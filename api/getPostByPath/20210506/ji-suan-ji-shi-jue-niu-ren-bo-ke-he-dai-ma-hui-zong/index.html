{"type":"getPostByPath","data":{"title":"计算机视觉牛人博客和代码汇总","date":"2021-05-06T12:24:57.000Z","description":"","categories":[],"tags":[{"name":"计算机视觉","_id":"clzpq9i0100dlsger4xt2hz7w"},{"name":"Computer Vision Resources","_id":"clzpq9i0100dosgergj44hc38"}],"content":"<p>转载自 <a href=\"https://www.cnblogs.com/findumars/p/5009003.html\">https://www.cnblogs.com/findumars/p/5009003.html</a> 略有删改，未经修正</p>\n<p>1 牛人Homepages（随意排序，不分先后）：</p>\n<p>1.<a href=\"http://iris.usc.edu/USC-Computer-Vision.html\" target=\"_blank\">USC Computer Vision Group</a>：南加大，多目标跟踪/检测等；</p>\n<p>2.<a href=\"http://www.vision.ee.ethz.ch/\" target=\"_blank\">ETHZ Computer Vision Laboratory</a>：苏黎世联邦理工学院，欧洲最好的几个CV/ML研究机构；</p>\n<p>3.<a href=\"http://www.vision.ee.ethz.ch/~hegrabne/\" target=\"_blank\">Helmut Grabner</a>：Online Boosting and Vision的作者，tracking by online feature selection的早期经典，貌似现在不是很活跃了，跑去创业了；</p>\n<p>4.<a href=\"http://www.cse.psu.edu/~rcollins/\" target=\"_blank\">Robert T. Collins</a>：PSU，也是跟踪界的大牛；</p>\n<p>5.<a href=\"http://users.eecs.northwestern.edu/~yingwu/\" target=\"_blank\">Ying Wu</a>：美国西北大学，华人学者中的翘楚；</p>\n<p>6.<a href=\"http://www3.ntu.edu.sg/home/jsyuan/\" target=\"_blank\">Junsong Yuan</a>：NTU，上面Wu老师的学生；</p>\n<p>7.<a href=\"http://www.cse.ohio-state.edu/~jwdavis/\" target=\"_blank\">James W. Davis</a>：俄亥俄州立，视频监控；</p>\n<p>8.&nbsp;<a href=\"http://www.acvt.com.au/\" target=\"_blank\">The Australian Centre for Visual Technologies</a>：阿德莱德大学的CV组，最近也是exceedingly active &amp; fruitful；</p>\n<p>9.<a href=\"http://cs.adelaide.edu.au/~chhshen/\" target=\"_blank\">Chunhua Shen</a>：属上面的ACVT组，最近非常活跃；</p>\n<p>10.<a href=\"http://cs.adelaide.edu.au/~xi/Xi_Li.html\" target=\"_blank\">Xi Li</a>：同属ACVT，之前是中科院的PHD，跟踪方面的论文很多，有理论深度；</p>\n<p>11.<a href=\"http://www.dabi.temple.edu/~hbling/\" target=\"_blank\">Haibin Ling</a>：天普大学，L1-Tracker及后续扩展，<strong>源码</strong>分享；</p>\n<p>12.<a href=\"http://lrs.icg.tugraz.at/index.php\" target=\"_blank\">Learning, Recognition, and Surveillance</a>：奥地利 TU Graz，在线学习，跟踪/检测等，active！<strong>源码</strong>分享；</p>\n<p>13.<a href=\"http://www.svcl.ucsd.edu/\" target=\"_blank\">Statistical Visual Computing Laboratory</a>：UCSD，光听名字就很学术吧，Saliency研究很有名；</p>\n<p>14.<a href=\"http://www.cs.toronto.edu/~dross/\" target=\"_blank\">David Ross</a>：多伦多大学，<a href=\"http://www.cs.toronto.edu/~dross/ivt/\" target=\"_blank\">IVT</a>的作者，跟踪中Generative表观的经典中的经典，提供<strong>源码</strong>，IVT的代码结构被后来很多人引用，值得一读；</p>\n<p>15.<a href=\"http://cvlab.epfl.ch/\" target=\"_blank\">EPFL, Computer Vision Laboratory</a>：洛桑理工的学院，和上面的的ETHZ CV lab同样是欧洲最好的CV研究大组；</p>\n<p>16.<a href=\"http://research.microsoft.com/en-US/people/jamiesho/default.aspx\" target=\"_blank\">Jamie Shotton</a>：属微软剑桥研究中心，<strong>Decision/Regression Forests</strong>；</p>\n<p>17.<a href=\"http://web.engr.oregonstate.edu/~sinisa/\" target=\"_blank\">Sinisa Todorovic</a>：俄勒冈州立，行为分析等；</p>\n<p>18.<a href=\"http://www.cis.upenn.edu/~jshi/\" target=\"_blank\">Shi Jianbo</a>：大名鼎鼎的Good Feature to Track作者，目前方向行为分析和多目标跟踪等；</p>\n<p>19.<a href=\"http://www.eng.tau.ac.il/~avidan/\" target=\"_blank\">Shai Avidan</a>：特拉维夫大学，大牛级，可算是Tracking-by-detection的开创者，Ensemble Tracking, SVM Tracking；</p>\n<p>20.<a href=\"http://vipl.ict.ac.cn/\" target=\"_blank\">Visual Information Processing and Learning</a>：中科院计算所，山世光老师的研究组，不需介绍了吧；</p>\n<p>21.<a href=\"http://www.eecs.qmul.ac.uk/~sgg/\" target=\"_blank\">Shaogang Gong</a>：Queen Mary University of London，各种PAMI，IJCV；</p>\n<p>22.<a href=\"http://www.patternrecognition.cn/~jian/\" target=\"_blank\">Yang Jian</a>：南京理工大学，2DPCA，人脸识别；</p>\n<p>23.<a href=\"http://groups.inf.ed.ac.uk/calvin/index.html\" target=\"_blank\">CALVIN</a>：weakly supervised learning，objectness；</p>\n<p>24.<a href=\"http://www.lv-nus.org/\" target=\"_blank\">Learning &amp; Vision Group</a>：NUS，稀疏表示；</p>\n<p>26.<a href=\"http://www.ee.cuhk.edu.hk/~xgwang/\" target=\"_blank\">Xiaogang Wang</a>：CUHK，active &amp; fruitful，行人检测，群体行为分析；</p>\n<p>27.<a href=\"http://mmlab.ie.cuhk.edu.hk/archive/profile/bolei/\" target=\"_blank\">Zhou, Bolei</a>：上面Wang老师硕士研究生，群体行为，看看人家的Publications已经轻松甩国内博士好几条街；</p>\n<p>28.<a href=\"http://vision.ics.uci.edu/index.html\" target=\"_blank\">Computational Vision Group</a>：Leader--<a href=\"http://www.ics.uci.edu/~dramanan/\" target=\"_blank\">Deva Ramanan</a>；</p>\n<p>29.<a href=\"http://www4.comp.polyu.edu.hk/~cslzhang/\" target=\"_blank\">Zhang Lei</a>：香港理工，稀疏表示，人脸识别，可以算大中华区比较活跃的研究组了，几乎每篇论文都有对应<strong>源码</strong>；</p>\n<p>30.<a href=\"http://www4.comp.polyu.edu.hk/~cskhzhang/\" target=\"_blank\">Zhang Kaihua</a>：上面Zhang老师学生，<a href=\"http://www4.comp.polyu.edu.hk/~cslzhang/CT/CT.htm\" target=\"_blank\">Compressive Tracking</a>；</p>\n<p>31.<a href=\"http://iris.usc.edu/people/pksharma/index.html\" target=\"_blank\">Pramod Sharma</a>：离线训练检测器的在线自适应，貌似是个不错的topic；</p>\n<p>32.<a href=\"http://www.lorisbazzani.info/#Pubs\" target=\"_blank\">Loris Bazzani</a>：person re-id，他的SDALF(<a href=\"http://www.lorisbazzani.info/code-datasets/sdalf-descriptor/\" target=\"_blank\">code</a>)描述子经常被用来做为比较对象，说明还是有参考价值的；</p>\n<p>33.<a href=\"http://blog.csdn.net/gxf1027/article/details/Felzenszwalb\" target=\"_blank\">Pedro Felzenszwalb</a>：布朗大学，目标检测，新新N人一枚；</p>\n<p>34.<a href=\"http://users.ece.cmu.edu/~kumar/\" target=\"_blank\">Vijayakumar Bhagavatula</a>：IEEE&nbsp;Fellow，&nbsp;correlation&nbsp;filters；</p>\n<p>35.<a href=\"http://homepage.tudelft.nl/19j49/Home.html\" target=\"_blank\">Laurens van der Maaten</a>：MLer.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>牛人主页（主页有很多论文代码）</strong></p>\n<div><a href=\"http://cseweb.ucsd.edu/~sjb/\" target=\"_blank\">Serge Belongie</a>&nbsp;at UC San Diego</div>\n<div><a href=\"http://web.mit.edu/torralba/www/\" target=\"_blank\">Antonio Torralba</a>&nbsp;at MIT</div>\n<div><a href=\"http://www.cs.cmu.edu/~efros/\" target=\"_blank\">Alexei Ffros</a>&nbsp;at CMU</div>\n<div><a href=\"http://people.csail.mit.edu/celiu/\" target=\"_blank\">Ce Liu</a>&nbsp;at Microsoft Research New England</div>\n<div><a href=\"http://www.vision.ee.ethz.ch/~calvin/\" target=\"_blank\">Vittorio Ferrari</a>&nbsp;at Univ.of&nbsp;Edinburgh</div>\n<div><a href=\"http://www.cs.utexas.edu/~grauman/\" target=\"_blank\">Kristen Grauman</a>&nbsp;at UT Austin</div>\n<div><a href=\"http://ttic.uchicago.edu/~dparikh/index.html\" target=\"_blank\">Devi Parikh</a>&nbsp;at&nbsp;&nbsp;TTI-Chicago&nbsp;(Marr Prize at ICCV2011)</div>\n<div><a href=\"http://www.columbia.edu/~jw2966/\" target=\"_blank\">John Wright</a>&nbsp;at Columbia Univ.</div>\n<div><a href=\"http://vision.ucsd.edu/~pdollar/\" target=\"_blank\">Piotr Dollar</a>&nbsp;at CalTech</div>\n<div><a href=\"http://vision.ucsd.edu/~bbabenko/\" target=\"_blank\">Boris Babenko</a>&nbsp;at UC San Diego</div>\n<div><a href=\"http://www.cs.toronto.edu/~dross/\" target=\"_blank\">David Ross</a>&nbsp;at Google/Youtube</div>\n<div><a href=\"http://www-stat.stanford.edu/~donoho/index.html\" target=\"_blank\">David Donoho</a>&nbsp;at Stanford Univ.</div>\n<div>&nbsp;</div>\n<div>&nbsp;</div>\n<div><strong>大神们：</strong></div>\n<div><strong>&nbsp;</strong></div>\n<div><a href=\"http://people.csail.mit.edu/billf/\" target=\"_blank\">William T. Freeman</a>&nbsp;at MIT</div>\n<div><a href=\"http://mi.eng.cam.ac.uk/~cipolla/index.htm\" target=\"_blank\">Roberto Cipolla</a>&nbsp;at Cambridge</div>\n<div><a href=\"http://www.cs.ubc.ca/~lowe/\" target=\"_blank\">David Lowe</a>&nbsp;at Univ. of British Columbia</div>\n<div><a href=\"http://server.cs.ucf.edu/~vision/faculty/shah.html\" target=\"_blank\">Mubarak Shah</a>&nbsp;at Univ. of Central Florida</div>\n<div><a href=\"http://yima.csl.illinois.edu/\" target=\"_blank\">Yi Ma</a>&nbsp;at MSRA</div>\n<div><a href=\"http://homes.esat.kuleuven.be/~tuytelaa/\" target=\"_blank\">Tinne Tuytelaars</a>&nbsp;at K.U. Leuven</div>\n<div><a href=\"http://www.eecs.berkeley.edu/~trevor/\" target=\"_blank\">Trevor Darrell</a>&nbsp;at U.C. Berkeley</div>\n<div><a href=\"http://www.cs.brown.edu/~black/\" target=\"_blank\">Michael J. Black</a>&nbsp;at Brown Univ.</div>\n<div><strong>&nbsp;</strong></div>\n<div><strong>&nbsp;</strong></div>\n<div><strong>&nbsp;</strong></div>\n<div><strong>&nbsp;</strong></div>\n<div><strong>重要研究组：</strong></div>\n<div><strong>&nbsp;</strong></div>\n<div><a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\" target=\"_blank\">Computer Vision Group</a>&nbsp;at UC Berkeley</div>\n<div><a href=\"http://www.robots.ox.ac.uk/\" target=\"_blank\">Robotics Research Group</a>&nbsp;at Univ. of Oxford</div>\n<div><a href=\"http://lear.inrialpes.fr/index.php\" target=\"_blank\">LEAR</a>&nbsp;at INRIA</div>\n<div><a href=\"http://vision.stanford.edu/index.html\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at Stanford</div>\n<div><a href=\"http://cvlab.epfl.ch/index.php\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at EPFL</div>\n<div><a href=\"http://www.vision.ee.ethz.ch/\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at ETH Zurich</div>\n<div><a href=\"http://cv.snu.ac.kr/\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at Seoul National Univ.</div>\n<div><a href=\"http://vision.ucsd.edu/\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at UC San Diego</div>\n<div><a href=\"http://vision.soe.ucsc.edu/\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at UC Santa Cruz</div>\n<div><a href=\"http://iris.usc.edu/USC-Computer-Vision.html\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at Univ. of Southern California</div>\n<div><a href=\"http://server.cs.ucf.edu/~vision/\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at Univ. of Central Florida</div>\n<div><a href=\"http://www1.cs.columbia.edu/CAVE/\" target=\"_blank\">Computer Vision Lab</a>&nbsp;at Columbia Univ.</div>\n<div><a href=\"http://www.vision.cs.ucla.edu/index.html\" target=\"_blank\">UCLA Vision Lab</a></div>\n<div><a href=\"http://masc.cs.gmu.edu/wiki/Home\" target=\"_blank\">Motion and Shape Computing Group</a>&nbsp;at George Mason Univ.</div>\n<div><a href=\"http://coewww.rutgers.edu/riul/\" target=\"_blank\">Robust Image Understanding Lab</a>&nbsp;at Rutgers Univ.</div>\n<div><a href=\"http://ivs.informatik.uni-bonn.de/index.html\" target=\"_blank\">Intelligent Vision Systems Group</a>&nbsp;at Univ. of Bonn</div>\n<div><a href=\"http://www.icg.tugraz.at/\" target=\"_blank\">Institute for Computer Graphics and Vision</a>&nbsp;at Graz Univ. of Tech.</div>\n<div><a href=\"http://www.caa.tuwien.ac.at/cvl/\" target=\"_blank\">Computer Vision Lab.</a>&nbsp;at Vienna Univ. of Tech.&nbsp;</div>\n<div><a href=\"http://www.cir.meduniwien.ac.at/\" target=\"_blank\">Computational Image Analysis and Radiology</a>&nbsp;at Medical Univ. of Vienna</div>\n<div><a href=\"http://personalrobotics.ri.cmu.edu/index.php\" target=\"_blank\"><strong>P</strong>ersonal&nbsp;<strong>R</strong>obotics&nbsp;<strong>L</strong>ab</a>&nbsp;at CMU</div>\n<div><a href=\"http://bigbird.psych.purdue.edu/index.html\" target=\"_blank\">Visual Perception Lab</a>&nbsp;at Purdue Univ.</div>\n<div><br>&nbsp;</div>\n<div>&nbsp;</div>\n<div><strong>潜力牛人：</strong></div>\n<div><strong>&nbsp;</strong></div>\n<div><a href=\"http://www.vision.ee.ethz.ch/~gallju/\" target=\"_blank\">Juergen Gall</a><em>&nbsp;</em>at<em>&nbsp;</em>ETH Zurich</div>\n<div><a href=\"http://www.cc.gatech.edu/~mflagg/\" target=\"_blank\">Matt Flagg</a>&nbsp;at Georgia Tech.</div>\n<div><a href=\"http://ttic.uchicago.edu/~salzmann/\" target=\"_blank\">Mathieu Salzmann</a>&nbsp;at TTI-Chicago</div>\n<div><a href=\"http://www.cs.brown.edu/~gregory/\" target=\"_blank\">Gerg Shakhnarovich</a>&nbsp;at TTI-Chicago</div>\n<div><a href=\"http://people.csail.mit.edu/taegsang/\" target=\"_blank\">Taeg Sang Cho</a>&nbsp;at MIT</div>\n<div><a href=\"http://www.ifp.illinois.edu/~jyang29/index.html\" target=\"_blank\">Jianchao Yang</a>&nbsp;at UIUC</div>\n<div><a href=\"http://www.gris.tu-darmstadt.de/~sroth/index.html\" target=\"_blank\">Stefan Roth</a>&nbsp;at TU Darmstadt</div>\n<div><a href=\"http://www.student.tugraz.at/peter.kontschieder/index.html\" target=\"_blank\">Peter Kontschieder</a>&nbsp;at Graz Univ. of Tech.</div>\n<div><a href=\"http://www.iai.uni-bonn.de/~kleind/\" target=\"_blank\">Dominik Alexander Klein</a>&nbsp;at Univ. of Bonn</div>\n<div><a href=\"http://www.cbsr.ia.ac.cn/users/ynyu/\" target=\"_blank\">Yinan Yu</a>&nbsp;at CASIA (PASCAL VOC 2010 Detection Challenge Winner)</div>\n<div><a href=\"http://info.ee.surrey.ac.uk/Personal/Z.Kalal/\" target=\"_blank\">Zdenek Kalal</a>&nbsp;at FPFL</div>\n<div><a href=\"http://cvlab.epfl.ch/~jpilet/\" target=\"_blank\">Julien Pilet</a>&nbsp;at FPFL</div>\n<div><a href=\"http://www.cs.ubc.ca/~okumak/index.html\" target=\"_blank\">Kenji Okuma</a></div>\n<div>&nbsp;</div>\n<div>2 个人、研究机构链接</div>\n<div>\n<p>（1）googleResearch；&nbsp;<a href=\"http://research.google.com/index.html\" target=\"_blank\">http://research.google.com/index.html</a><br>（2）MIT博士，汤晓欧学生林达华；<a href=\"http://people.csail.mit.edu/dhlin/index.html\" target=\"_blank\">http://people.csail.mit.edu/dhlin/index.html</a><br>（3）MIT博士后Douglas Lanman；&nbsp;<a href=\"http://web.media.mit.edu/~dlanman/\" target=\"_blank\">http://web.media.mit.edu/~dlanman/</a><br>（4）opencv中文网站；<a href=\"http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5\" target=\"_blank\">http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5</a><br>（5）Stanford大学vision实验室；&nbsp;<a href=\"http://vision.stanford.edu/research.html\" target=\"_blank\">http://vision.stanford.edu/research.html</a><br>（6）Stanford大学博士崔靖宇；&nbsp;<a href=\"http://www.stanford.edu/~jycui/\" target=\"_blank\">http://www.stanford.edu/~jycui/</a><br>（7）UCLA教授朱松纯；&nbsp;<a href=\"http://www.stat.ucla.edu/~sczhu/\" target=\"_blank\">http://www.stat.ucla.edu/~sczhu/</a><br>（8）中国人工智能网；&nbsp;<a href=\"http://www.chinaai.org/\" target=\"_blank\">http://www.chinaai.org/</a><br>（9）中国视觉网；&nbsp;<a href=\"http://www.china-vision.net/\" target=\"_blank\">http://www.china-vision.net/</a><br>（10）中科院自动化所；&nbsp;<a href=\"http://www.ia.cas.cn/\" target=\"_blank\">http://www.ia.cas.cn/</a><br>（11）中科院自动化所李子青研究员；&nbsp;<a href=\"http://www.cbsr.ia.ac.cn/users/szli/\" target=\"_blank\">http://www.cbsr.ia.ac.cn/users/szli/</a><br>（12）中科院计算所山世光研究员；&nbsp;<a href=\"http://www.jdl.ac.cn/user/sgshan/\" target=\"_blank\">http://www.jdl.ac.cn/user/sgshan/</a><br>（13）人脸识别主页；&nbsp;<a href=\"http://www.face-rec.org/\" target=\"_blank\">http://www.face-rec.org/</a><br>（14）加州大学伯克利分校CV小组；<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/\" target=\"_blank\">http://www.eecs.berkeley.edu/Research/Projects/CS/vision/</a></p>\n<p>（15）南加州大学CV实验室；&nbsp;<a href=\"http://iris.usc.edu/USC-Computer-Vision.html\" target=\"_blank\">http://iris.usc.edu/USC-Computer-Vision.html</a><br>（16）卡内基梅隆大学CV主页；</p>\n<p><a href=\"http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html\" target=\"_blank\">http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html</a></p>\n<p>（17）微软CV研究员Richard Szeliski；<a href=\"http://research.microsoft.com/en-us/um/people/szeliski/\" target=\"_blank\">http://research.microsoft.com/en-us/um/people/szeliski/</a><br>（18）微软亚洲研究院计算机视觉研究组；&nbsp;<a href=\"http://research.microsoft.com/en-us/groups/vc/\" target=\"_blank\">http://research.microsoft.com/en-us/groups/vc/</a><br>（19）微软剑桥研究院ML与CV研究组；&nbsp;<a href=\"http://research.microsoft.com/en-us/groups/mlp/default.aspx\" target=\"_blank\">http://research.microsoft.com/en-us/groups/mlp/default.aspx</a></p>\n<p>（20）研学论坛；&nbsp;<a href=\"http://bbs.matwav.com/\" target=\"_blank\">http://bbs.matwav.com/</a><br>（21）美国Rutgers大学助理教授刘青山；<a href=\"http://www.research.rutgers.edu/~qsliu/\" target=\"_blank\">http://www.research.rutgers.edu/~qsliu/</a><br>（22）计算机视觉最新资讯网；&nbsp;<a href=\"http://www.cvchina.info/\" target=\"_blank\">http://www.cvchina.info/</a><br>（23）运动检测、阴影、跟踪的测试视频下载；<a href=\"http://apps.hi.baidu.com/share/detail/18903287\" target=\"_blank\">http://apps.hi.baidu.com/share/detail/18903287</a><br>（24）香港中文大学助理教授王晓刚；&nbsp;<a href=\"http://www.ee.cuhk.edu.hk/~xgwang/\" target=\"_blank\">http://www.ee.cuhk.edu.hk/~xgwang/</a><br>(25)香港中文大学多媒体实验室（汤晓鸥）;&nbsp;<a href=\"http://mmlab.ie.cuhk.edu.hk/\" target=\"_blank\">http://mmlab.ie.cuhk.edu.hk/</a><br>(26)U.C. San Diego. computer vision;<a href=\"http://vision.ucsd.edu/content/home\" target=\"_blank\">http://vision.ucsd.edu/content/home</a><br>(27)CVonline;&nbsp;<a href=\"http://homepages.inf.ed.ac.uk/rbf/CVonline/\" target=\"_blank\">http://homepages.inf.ed.ac.uk/rbf/CVonline/</a><br>(28)computer vision software;&nbsp;<a href=\"http://peipa.essex.ac.uk/info/software.html\" target=\"_blank\">http://peipa.essex.ac.uk/info/software.html</a><br>(29)Computer Vision Resource;&nbsp;<a href=\"http://www.cvpapers.com/\" target=\"_blank\">http://www.cvpapers.com/</a><br>(30)computer vision research groups;<a href=\"http://peipa.essex.ac.uk/info/groups.html\" target=\"_blank\">http://peipa.essex.ac.uk/info/groups.html</a><br>(31)computer vision center;&nbsp;<a href=\"http://computervisioncentral.com/cvcnews\" target=\"_blank\">http://computervisioncentral.com/cvcnews</a></p>\n<p>(32)浙江大学图像技术研究与应用（ITRA）团队：<a href=\"http://www.dvzju.com/\" target=\"_blank\">http://www.dvzju.com/</a></p>\n<p>(33)自动识别网：<a href=\"http://www.autoid-china.com.cn/\" target=\"_blank\">http://www.autoid-china.com.cn/</a></p>\n<p>(34)清华大学章毓晋教授：<a href=\"http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html\" target=\"_blank\">http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html</a></p>\n<p>(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:<a href=\"http://www.willowgarage.com/\" target=\"_blank\">http://www.willowgarage.com/</a></p>\n<p>(36)上海交通大学图像处理与模式识别研究所：<a href=\"http://www.pami.sjtu.edu.cn/\" target=\"_blank\">http://www.pami.sjtu.edu.cn/</a></p>\n<p>(37)上海交通大学计算机视觉实验室刘允才教授：<a href=\"http://www.visionlab.sjtu.edu.cn/\" target=\"_blank\">http://www.visionlab.sjtu.edu.cn/</a></p>\n<p>(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：<a href=\"http://www.cs.utexas.edu/~grauman/\" target=\"_blank\">http://www.cs.utexas.edu/~grauman/</a>&nbsp;图像分解，检索</p>\n<p>(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：<a href=\"http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp\" target=\"_blank\">http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp</a></p>\n<p>(40)北京大学高文教授：<a href=\"http://www.jdl.ac.cn/htm-gaowen/\" target=\"_blank\">http://www.jdl.ac.cn/htm-gaowen/</a></p>\n<p>(41)清华大学艾海舟教授：<a href=\"http://media.cs.tsinghua.edu.cn/cn/aihz\" target=\"_blank\">http://media.cs.tsinghua.edu.cn/cn/aihz</a></p>\n<p>(42)中科院生物识别与安全技术研究中心：<a href=\"http://www.cbsr.ia.ac.cn/china/index%20CH.asp\" target=\"_blank\">http://www.cbsr.ia.ac.cn/china/index%20CH.asp</a></p>\n<p>(43)瑞士巴塞尔大学 Thomas Vetter教授：<a href=\"http://informatik.unibas.ch/personen/vetter_t.html\" target=\"_blank\">http://informatik.unibas.ch/personen/vetter_t.html</a></p>\n<p>(44)<span class=\"st\">俄勒冈州立大学 Rob Hess博士：<a href=\"http://blogs.oregonstate.edu/hess/\" target=\"_blank\">http://blogs.oregonstate.edu/hess/</a></span></p>\n<p>(45)深圳大学 于仕祺副教授：<a href=\"http://yushiqi.cn/\" target=\"_blank\">http://yushiqi.cn/</a></p>\n<p>(46)西安交通大学人工智能与机器人研究所：<a href=\"http://www.aiar.xjtu.edu.cn/\" target=\"_blank\">http://www.aiar.xjtu.edu.cn/</a></p>\n<p>(47)卡内基梅隆大学研究员Robert T. Collins:<a href=\"http://www.cs.cmu.edu/~rcollins/home.html#Background\" target=\"_blank\">http://www.cs.cmu.edu/~rcollins/home.html#Background</a></p>\n<p>(48)MIT博士Chris Stauffer:<a href=\"http://people.csail.mit.edu/stauffer/Home/index.php\" target=\"_blank\">http://people.csail.mit.edu/stauffer/Home/index.php</a></p>\n<p>(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：<a href=\"http://www.cse.msu.edu/rgroups/biometrics/\" target=\"_blank\">http://www.cse.msu.edu/rgroups/biometrics/</a></p>\n<p>(50)美国伊利诺伊州立大学Thomas S. Huang:<a href=\"http://www.beckman.illinois.edu/directory/t-huang1\" target=\"_blank\">http://www.beckman.illinois.edu/directory/t-huang1</a></p>\n<p>(51)武汉大学数字摄影测量与计算机视觉研究中心：<a href=\"http://www.whudpcv.cn/index.asp\" target=\"_blank\">http://www.whudpcv.cn/index.asp</a></p>\n<p>(52)瑞士巴塞尔大学Sami Romdhani助理研究员：<a href=\"http://informatik.unibas.ch/personen/romdhani_sami/\" target=\"_blank\">http://informatik.unibas.ch/personen/romdhani_sami/</a></p>\n<p>(53)CMU大学研究员Yang Wang:<a href=\"http://www.cs.cmu.edu/~wangy/home.html\" target=\"_blank\">http://www.cs.cmu.edu/~wangy/home.html</a></p>\n<p>(54)英国曼彻斯特大学Tim Cootes教授：<a href=\"http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/\" target=\"_blank\">http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/</a></p>\n<p>(55)美国罗彻斯特大学教授Jiebo Luo:<a href=\"http://www.cs.rochester.edu/u/jluo/\" target=\"_blank\">http://www.cs.rochester.edu/u/jluo/</a></p>\n<p>(56)美国普渡大学机器人视觉实验室：<a href=\"https://engineering.purdue.edu/RVL/Welcome.html\" target=\"_blank\">https://engineering.purdue.edu/RVL/Welcome.html</a></p>\n<p>(57)美国宾利州立大学感知、运动与认识实验室：<a href=\"http://vision.cse.psu.edu/home/home.shtml\" target=\"_blank\">http://vision.cse.psu.edu/home/home.shtml</a></p>\n<p>(58)美国宾夕法尼亚大学GRASP实验室：<a href=\"https://www.grasp.upenn.edu/\" target=\"_blank\">https://www.grasp.upenn.edu/</a></p>\n<p>(59)美国内达华大学里诺校区CV实验室：<a href=\"http://www.cse.unr.edu/CVL/index.php\" target=\"_blank\">http://www.cse.unr.edu/CVL/index.php</a></p>\n<p>(60)美国密西根大学vision实验室：<a href=\"http://www.eecs.umich.edu/vision/index.html\" target=\"_blank\">http://www.eecs.umich.edu/vision/index.html</a></p>\n<p>(61)University of Massachusetts(麻省大学),视觉实验室：<a href=\"http://vis-www.cs.umass.edu/index.html\" target=\"_blank\">http://vis-www.cs.umass.edu/index.html</a></p>\n<p>(62)华盛顿大学博士后Iva Kemelmacher:<a href=\"http://www.cs.washington.edu/homes/kemelmi\" target=\"_blank\">http://www.cs.washington.edu/homes/kemelmi</a></p>\n<p>(63)以色列魏茨曼科技大学Ronen Basri:<a href=\"http://www.wisdom.weizmann.ac.il/~ronen/index.html\" target=\"_blank\">http://www.wisdom.weizmann.ac.il/~ronen/index.html</a></p>\n<p>(64)瑞士ETH-Zurich大学CV实验室：<a href=\"http://www.vision.ee.ethz.ch/boostingTrackers/index.htm\" target=\"_blank\">http://www.vision.ee.ethz.ch/boostingTrackers/index.htm</a></p>\n<p>(65)微软CV研究员张正友：<a href=\"http://research.microsoft.com/en-us/um/people/zhang/\" target=\"_blank\">http://research.microsoft.com/en-us/um/people/zhang/</a></p>\n<p>(66)中科院自动化所医学影像研究室：<a href=\"http://www.3dmed.net/\" target=\"_blank\">http://www.3dmed.net/</a></p>\n<p>(67)中科院田捷研究员：<a href=\"http://www.3dmed.net/tian/\" target=\"_blank\">http://www.3dmed.net/tian/</a></p>\n<p>(68)微软Redmond研究院研究员Simon Baker:<a href=\"http://research.microsoft.com/en-us/people/sbaker/\" target=\"_blank\">http://research.microsoft.com/en-us/people/sbaker/</a></p>\n<p>(69)普林斯顿大学教授李凯：<a href=\"http://www.cs.princeton.edu/~li/\" target=\"_blank\">http://www.cs.princeton.edu/~li/</a><br>(70)普林斯顿大学博士贾登：<a href=\"http://www.cs.princeton.edu/~jiadeng/\" target=\"_blank\">http://www.cs.princeton.edu/~jiadeng/</a><br>(71)牛津大学教授Andrew Zisserman：&nbsp;<a href=\"http://www.robots.ox.ac.uk/~az/\" target=\"_blank\">http://www.robots.ox.ac.uk/~az/</a><br>(72)英国leeds大学研究员Mark Everingham:<a href=\"http://www.comp.leeds.ac.uk/me/\" target=\"_blank\">http://www.comp.leeds.ac.uk/me/</a><br>(73)英国爱丁堡大学教授Chris William:&nbsp;<a href=\"http://homepages.inf.ed.ac.uk/ckiw/\" target=\"_blank\">http://homepages.inf.ed.ac.uk/ckiw/</a><br>(74)微软剑桥研究院研究员John Winn:&nbsp;<a href=\"http://johnwinn.org/\" target=\"_blank\">http://johnwinn.org/</a><br>(75)佐治亚理工学院教授Monson H.Hayes：<a href=\"http://savannah.gatech.edu/people/mhayes/index.html\" target=\"_blank\">http://savannah.gatech.edu/people/mhayes/index.html</a><br>(76)微软亚洲研究院研究员孙剑：<a href=\"http://research.microsoft.com/en-us/people/jiansun/\" target=\"_blank\">http://research.microsoft.com/en-us/people/jiansun/</a><br>(77)微软亚洲研究院研究员马毅：<a href=\"http://research.microsoft.com/en-us/people/mayi/\" target=\"_blank\">http://research.microsoft.com/en-us/people/mayi/</a><br>(78)英国哥伦比亚大学教授David Lowe:&nbsp;<a href=\"http://www.cs.ubc.ca/~lowe/\" target=\"_blank\">http://www.cs.ubc.ca/~lowe/</a><br>(79)英国爱丁堡大学教授Bob Fisher:&nbsp;<a href=\"http://homepages.inf.ed.ac.uk/rbf/\" target=\"_blank\">http://homepages.inf.ed.ac.uk/rbf/</a><br>(80)加州大学圣地亚哥分校教授Serge J.Belongie:<a href=\"http://cseweb.ucsd.edu/~sjb/\" target=\"_blank\">http://cseweb.ucsd.edu/~sjb/</a><br>(81)威斯康星大学教授Charles R.Dyer:&nbsp;<a href=\"http://pages.cs.wisc.edu/~dyer/\" target=\"_blank\">http://pages.cs.wisc.edu/~dyer/</a><br>(82)多伦多大学教授Allan.Jepson:&nbsp;<a href=\"http://www.cs.toronto.edu/~jepson/\" target=\"_blank\">http://www.cs.toronto.edu/~jepson/</a><br>(83)伦斯勒理工学院教授Qiang Ji:&nbsp;<a href=\"http://www.ecse.rpi.edu/~qji/\" target=\"_blank\">http://www.ecse.rpi.edu/~qji/</a><br>(84)CMU研究员Daniel Huber:&nbsp;<a href=\"http://www.ri.cmu.edu/person.html?person_id=123\" target=\"_blank\">http://www.ri.cmu.edu/person.html?person_id=123</a><br>(85)多伦多大学教授：David J.Fleet:&nbsp;<a href=\"http://www.cs.toronto.edu/~fleet/\" target=\"_blank\">http://www.cs.toronto.edu/~fleet/</a><br>(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:<a href=\"http://www.eecs.qmul.ac.uk/~andrea/\" target=\"_blank\">http://www.eecs.qmul.ac.uk/~andrea/</a><br>(87)多伦多大学教授Kyros Kutulakos:&nbsp;<a href=\"http://www.cs.toronto.edu/~kyros/\" target=\"_blank\">http://www.cs.toronto.edu/~kyros/</a><br>(88)杜克大学教授Carlo Tomasi:&nbsp;<a href=\"http://www.cs.duke.edu/~tomasi/\" target=\"_blank\">http://www.cs.duke.edu/~tomasi/</a><br>(89)CMU教授Martial Hebert:&nbsp;<a href=\"http://www.cs.cmu.edu/~hebert/\" target=\"_blank\">http://www.cs.cmu.edu/~hebert/</a><br>(90)MIT助理教授Antonio Torralba:&nbsp;<a href=\"http://web.mit.edu/torralba/www/\" target=\"_blank\">http://web.mit.edu/torralba/www/</a><br>(91)马里兰大学研究员Yasel Yacoob:&nbsp;<a href=\"http://www.umiacs.umd.edu/users/yaser/\" target=\"_blank\">http://www.umiacs.umd.edu/users/yaser/</a><br>(92)康奈尔大学教授Ramin Zabih:&nbsp;<a href=\"http://www.cs.cornell.edu/~rdz/\" target=\"_blank\">http://www.cs.cornell.edu/~rdz/</a></p>\n<p>(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/<br>(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/<br>(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/<br>(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/<br>(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/<br>(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/<br>(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm<br>(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html<br>(101)瑞士联邦理工学院博士后Helmut Grabner:<a href=\"http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV\" target=\"_blank\">http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV</a></p>\n<p>(102)香港中文大学教授贾佳亚：<a href=\"http://www.cse.cuhk.edu.hk/~leojia/index.html\" target=\"_blank\">http://www.cse.cuhk.edu.hk/~leojia/index.html</a></p>\n<p>(103)南京大学教授吴建鑫：<a href=\"http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\" target=\"_blank\">http://c2inet.sce.ntu.edu.sg/Jianxin/index.html</a></p>\n<p>(104)GE研究院研究员李关：<a href=\"http://www.cs.unc.edu/~lguan/\" target=\"_blank\">http://www.cs.unc.edu/~lguan/</a></p>\n<p>(105)佐治亚理工学院教授Monson Hayes:<a href=\"http://savannah.gatech.edu/people/mhayes/\" target=\"_blank\">http://savannah.gatech.edu/people/mhayes/</a></p>\n<p>(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):<a href=\"http://pascallin.ecs.soton.ac.uk/challenges/VOC/\" target=\"_blank\">http://pascallin.ecs.soton.ac.uk/challenges/VOC/</a></p>\n<p>(107)机器视觉开源处理库汇总：<a href=\"http://archive.cnblogs.com/a/2217609/\" target=\"_blank\">http://archive.cnblogs.com/a/2217609/</a></p>\n<p>(108)布朗大学教授Benjamin Kimia:&nbsp;<a href=\"http://www.lems.brown.edu/kimia.html\" target=\"_blank\">http://www.lems.brown.edu/kimia.html</a>&nbsp;</p>\n<p>(109)数据堂-图像处理相关的样本数据：<a href=\"http://www.datatang.com/data/list/602026/p1\" target=\"_blank\">http://www.datatang.com/data/list/602026/p1</a></p>\n<p>(110)东软基于CV的汽车辅助驾驶系统：<a href=\"http://www.neusoft.com/cn/solutions/1047/\" target=\"_blank\">http://www.neusoft.com/cn/solutions/1047/</a></p>\n<p>(111)马里兰大学教授Rema Chellappa:<a href=\"http://www.cfar.umd.edu/~rama/\" target=\"_blank\">http://www.cfar.umd.edu/~rama/</a></p>\n<p>(112)芝加哥丰田研究中心助理教授Devi Parikh：<a href=\"http://ttic.uchicago.edu/~dparikh/index.html\" target=\"_blank\">http://ttic.uchicago.edu/~dparikh/index.html</a></p>\n<p>(113)宾夕法尼亚大学助理教授石建波：<a href=\"http://www.cis.upenn.edu/~jshi/\" target=\"_blank\">http://www.cis.upenn.edu/~jshi/</a></p>\n<p>(114)比利时鲁汶大学教授Luc Van Gool：<a href=\"http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1\" target=\"_blank\">http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1</a>,&nbsp;<a href=\"http://www.vision.ee.ethz.ch/~vangool/\" target=\"_blank\">http://www.vision.ee.ethz.ch/~vangool/</a></p>\n<p>(115)行人检测主页：<a href=\"http://www.pedestrian-detection.com/\" target=\"_blank\">http://www.pedestrian-detection.com/</a></p>\n<p>(116)法国学习算法与系统实验室Basilio Noris博士：<a href=\"http://lasa.epfl.ch/people/member.php?SCIPER=129576\" target=\"_blank\">http://lasa.epfl.ch/people/member.php?SCIPER=129576</a>&nbsp;<a href=\"http://mldemos.epfl.ch/\" target=\"_blank\">http://mldemos.epfl.ch/</a></p>\n<p>(117)美国马里兰大学LARRY S.DAVIS教授：<a href=\"http://www.umiacs.umd.edu/~lsd/\" target=\"_blank\">http://www.umiacs.umd.edu/~lsd/</a></p>\n<p>(118)计算机视觉论文分类导航：<a href=\"http://www.visionbib.com/bibliography/contents.html\" target=\"_blank\">http://www.visionbib.com/bibliography/contents.html</a></p>\n<p>(119)计算机视觉分类信息导航：<a href=\"http://www.visionbib.com/\" target=\"_blank\">http://www.visionbib.com/</a></p>\n<p>(120)西班牙马德里理工大学博士Marcos Nieto：<a href=\"http://marcosnieto.net/\" target=\"_blank\">http://marcosnieto.net/</a></p>\n<p>(121)香港理工大学副教授张磊：<a href=\"http://www4.comp.polyu.edu.hk/~cslzhang/\" target=\"_blank\">http://www4.comp.polyu.edu.hk/~cslzhang/</a></p>\n<p>(122)以色列技术学院教授Michael Elad：<a href=\"http://www.cs.technion.ac.il/~elad/\" target=\"_blank\">http://www.cs.technion.ac.il/~elad/</a></p>\n<p>(123)韩国启明大学计算机视觉与模式识别实验室：<a href=\"http://cvpr.kmu.ac.kr/\" target=\"_blank\">http://cvpr.kmu.ac.kr/</a></p>\n<p>(124)英国诺丁汉大学Michel Valstar博士：<a href=\"http://www.cs.nott.ac.uk/~mfv/\" target=\"_blank\">http://www.cs.nott.ac.uk/~mfv/</a></p>\n<p>(125)卡内基梅隆大学Takeo Kanade教授:<a href=\"http://www.ri.cmu.edu/people/kanade_takeo.html\" target=\"_blank\">http://www.ri.cmu.edu/people/kanade_takeo.html</a></p>\n<p>(126)微软学术搜索：<a href=\"http://libra.msra.cn/\" target=\"_blank\">http://libra.msra.cn/</a></p>\n<p>(127)比利时天主教鲁汶大学Radu Timofte博士：<a href=\"http://homes.esat.kuleuven.be/~rtimofte/\" target=\"_blank\">http://homes.esat.kuleuven.be/~rtimofte/</a>，交通标志检测，定位，3D跟踪</p>\n<p>(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:<a href=\"http://www.iainm.com/iainm/Home.html\" target=\"_blank\">http://www.iainm.com/iainm/Home.html</a></p>\n<p><a href=\"http://www.ri.cmu.edu/person.html?type=publications&amp;person_id=741\" target=\"_blank\">http://www.ri.cmu.edu/person.html?type=publications&amp;person_id=741</a>&nbsp;AAM,三维重建</p>\n<p>（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理</p>\n<p>（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索<br>（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理</p>\n<p>（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位</p>\n<p>（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/ &nbsp; 人脸识别、监控、3D、视频检索、<br>（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建</p>\n<p>（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、</p>\n<p>（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类</p>\n<p>（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html&nbsp;质量评判、水印、稀疏表示、超分辨率</p>\n<p>（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习</p>\n<p>（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/</p>\n<p>（140）微软Redmond研究院研究员Piotr Dollar:&nbsp;http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、</p>\n<p>（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛</p>\n<p>（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习</p>\n<p>（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习</p>\n<p>（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/ &nbsp;人的姿态估计和跟踪</p>\n<p>（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别</p>\n<p>（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或&nbsp;http://www.pauldebevec.com/&nbsp;将CV和CG结合研究&nbsp;人脸捕捉重建技术</p>\n<p>（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建</p>\n<p>（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/&nbsp;跟踪和机器人导航</p>\n<p>（149）CMU大学Alyosha Efros 教授:&nbsp;https://www.cs.cmu.edu/~efros/ 图像纹理合成</p>\n<p>（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/&nbsp;轮廓检测、图像/视频分割、图形匹配、目标识别</p>\n<p>（151）MIT教授William Freeman：&nbsp;http://people.csail.mit.edu/billf/ 图像纹理合成</p>\n<p>（152）CMU博士Henry Schneiderman：&nbsp;http://www.cs.cmu.edu/~hws/&nbsp;目标检测和识别；</p>\n<p>（153）微软研究员Paul Viola:&nbsp;http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法</p>\n<p>（154）微软研究员Antonio Criminisi:&nbsp;http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；</p>\n<p>（155）魏茨曼科学研究所教授Michal Irani:&nbsp;http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率</p>\n<p>（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实</p>\n<p>（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别</p>\n<p>（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；</p>\n<p>（159）斯坦福大学Sebastian Thrun教授：<a href=\"http://robots.stanford.edu/index.html\" target=\"_blank\">http://robots.stanford.edu/index.html</a>&nbsp;机器人；</p>\n<p>（160）多伦多大学Geoffrey E.Hinton教授：<a href=\"http://www.cs.toronto.edu/~hinton/\" target=\"_blank\">http://www.cs.toronto.edu/~hinton/</a>&nbsp;深度学习</p>\n<p>（161）凤巢系统架构师张栋博士：<a href=\"http://weibo.com/machinelearning\" target=\"_blank\">http://weibo.com/machinelearning</a></p>\n<p>（162）2012年龙星计划机器学习课程：<a href=\"http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html\" target=\"_blank\">http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html</a></p>\n<p>（163）中科院自动化所肖柏华教授：<a href=\"http://www.compsys.ia.ac.cn/people/xiaobaihua.html\" target=\"_blank\">http://www.compsys.ia.ac.cn/people/xiaobaihua.html</a>&nbsp;文字识别、人脸识别、质量评判</p>\n<p>（164）图像视频质量评判：<a href=\"http://live.ece.utexas.edu/research/quality/\" target=\"_blank\">http://live.ece.utexas.edu/research/quality/</a></p>\n<p>（165）纽约大学Yann LeCun教授<a href=\"http://yann.lecun.com/\" target=\"_blank\">http://yann.lecun.com/</a>&nbsp; &nbsp;<a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">http://yann.lecun.com/exdb/mnist/</a>&nbsp; 手写体数字识别</p>\n<p>（166）二维条码识别开源库zxing：<a href=\"http://code.google.com/p/zxing/\" target=\"_blank\">http://code.google.com/p/zxing/</a></p>\n<p>（167）布朗大学Pedro Felzenszwalb教授：<a href=\"http://cs.brown.edu/~pff/\" target=\"_blank\">http://cs.brown.edu/~pff/</a>&nbsp;特征提取，Deformable Part Model</p>\n<p>（168）伊利诺伊香槟大学Svetlana Lazebnik教授：<a href=\"http://www.cs.illinois.edu/homes/slazebni/\" target=\"_blank\">http://www.cs.illinois.edu/homes/slazebni/</a>&nbsp;特征提取，聚类，图像检索</p>\n<p>（169）荷兰乌德勒支大学图像与多媒体研究中心<a href=\"http://www.cs.uu.nl/centers/give/multimedia/index.html\" target=\"_blank\">http://www.cs.uu.nl/centers/give/multimedia/index.html</a>&nbsp;图像、多媒体检索与匹配</p>\n<p>（170）英国格拉斯哥大学信息检索小组：<a href=\"http://ir.dcs.gla.ac.uk/\" target=\"_blank\">http://ir.dcs.gla.ac.uk/</a>&nbsp;文本、图像、视频检索</p>\n<p>（171）中科院自动化所孙哲南助理教书：<a href=\"http://www.cbsr.ia.ac.cn/users/znsun/\" target=\"_blank\">http://www.cbsr.ia.ac.cn/users/znsun/</a>&nbsp;虹膜识别、掌纹识别、人脸识别</p>\n<p>（172）南京信息工程大学刘青山教授：<a href=\"http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096\" target=\"_blank\">http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096</a>&nbsp;人脸图像分析、医学图像分析</p>\n<p>（173）清华大学助理教授冯建江：<a href=\"http://ivg.au.tsinghua.edu.cn/~jfeng/\" target=\"_blank\">http://ivg.au.tsinghua.edu.cn/~jfeng/</a>&nbsp;指纹识别</p>\n<p>（174）北航助理教授黄迪：<a href=\"http://irip.buaa.edu.cn/~dihuang/\" target=\"_blank\">http://irip.buaa.edu.cn/~dihuang/</a>&nbsp;3D人脸识别</p>\n<p>（175）中山大学助理教授郑伟诗：<a href=\"http://sist.sysu.edu.cn/~zhwshi/\" target=\"_blank\">http://sist.sysu.edu.cn/~zhwshi/</a>&nbsp;人脸识别、特征匹配、聚类、检索；</p>\n<p>（176）google瑞士苏黎世的工程师Thomas Deselaers:&nbsp;<a href=\"http://thomas.deselaers.de/index.html\" target=\"_blank\">http://thomas.deselaers.de/index.html</a>&nbsp;图像检索</p>\n<p>（177）百度深度学习研究中心博士后余轶南：<a href=\"http://www.cbsr.ia.ac.cn/users/ynyu/index.htm\" target=\"_blank\">http://www.cbsr.ia.ac.cn/users/ynyu/index.htm</a>&nbsp;目标检测，图像检索</p>\n<p>（178）威兹曼科技大学超分辨率：<a href=\"http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html\" target=\"_blank\">http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html</a></p>\n<p>（179）德克萨斯大学奥斯汀分校Al Bovik教授：<a href=\"http://live.ece.utexas.edu/people/bovik/\" target=\"_blank\">http://live.ece.utexas.edu/people/bovik/</a>&nbsp;图像视频质量判别、特征提取</p>\n<p>（180）以色列希伯来大学Yair Weiss教授：<a href=\"http://www.cs.huji.ac.il/~yweiss/\" target=\"_blank\">http://www.cs.huji.ac.il/~yweiss/</a>&nbsp;机器学习、超分辨率</p>\n<p>（181）以色列希伯来大学Daniel Zoran博士：<a href=\"http://www.cs.huji.ac.il/~daniez/\" target=\"_blank\">http://www.cs.huji.ac.il/~daniez/</a>&nbsp;超分辨率、去噪</p>\n<p>（182）美国加州大学Peyman Milanfar教授：<a href=\"http://users.soe.ucsc.edu/~milanfar/\" target=\"_blank\">http://users.soe.ucsc.edu/~milanfar/</a>&nbsp;去噪</p>\n<p>（183）中科院计算所副研究员常虹：<a href=\"http://www.jdl.ac.cn/user/hchang/index.html\" target=\"_blank\">http://www.jdl.ac.cn/user/hchang/index.html</a>&nbsp;图像检索、半监督学习、超分辨率</p>\n<p>（184）以色列威茨曼大学Anat Levin教授：<a href=\"http://www.wisdom.weizmann.ac.il/~levina/\" target=\"_blank\">http://www.wisdom.weizmann.ac.il/~levina/</a>&nbsp;去噪、去模糊</p>\n<p>（185）以色列威茨曼大学Daniel Glasner博士后：<a href=\"http://www.wisdom.weizmann.ac.il/~glasner/\" target=\"_blank\">http://www.wisdom.weizmann.ac.il/~glasner/</a>&nbsp;超分辨率、分割、姿态估计</p>\n<p>（186）密西根大学助理教授Honglak Lee:&nbsp;<a href=\"http://web.eecs.umich.edu/~honglak/\" target=\"_blank\">http://web.eecs.umich.edu/~honglak/</a>&nbsp;机器学习、特征提取，去噪、稀疏表示；</p>\n<p>（187）MIT周博磊博士：<a href=\"http://people.csail.mit.edu/bzhou/\" target=\"_blank\">http://people.csail.mit.edu/bzhou/</a>&nbsp;聚集分析、运动检测</p>\n<p>（188）美国田纳西大学Li He博士：<a href=\"http://web.eecs.utk.edu/~lhe4/\" target=\"_blank\">http://web.eecs.utk.edu/~lhe4/</a>&nbsp;稀疏表示、超分辨率；</p>\n<p>（189）Adobe研究院Jianchao Yang研究员：<a href=\"http://www.ifp.illinois.edu/~jyang29/\" target=\"_blank\">http://www.ifp.illinois.edu/~jyang29/</a>&nbsp;稀疏表示，超分辨率、图片检索、去噪、去模糊</p>\n<p>（190）Deep Learning主页：<a href=\"http://deeplearning.net/\" target=\"_blank\">http://deeplearning.net/</a>&nbsp;深度学习论文、软件，代码，demo，数据等；</p>\n<p>（191）斯坦福大学Andrew Ng教授：<a href=\"http://cs.stanford.edu/people/ang/\" target=\"_blank\">http://cs.stanford.edu/people/ang/</a>&nbsp;深度神经网络，深度学习</p>\n<p>（192）Elefant:&nbsp;<a href=\"http://elefant.developer.nicta.com.au/\" target=\"_blank\">http://elefant.developer.nicta.com.au/</a>&nbsp;机器学习开源库</p>\n<p>（193）微软研究员Ce Liu:&nbsp;<a href=\"http://people.csail.mit.edu/celiu/\" target=\"_blank\">http://people.csail.mit.edu/celiu/</a>&nbsp;去噪、超分辨率、去模糊、分割</p>\n<p>（194）West Virginia大学助理教授Xin Li:&nbsp;<a href=\"http://www.csee.wvu.edu/~xinl/\" target=\"_blank\">http://www.csee.wvu.edu/~xinl/</a>&nbsp;边缘检测、降噪、去模糊</p>\n<p>（195）<a href=\"http://www.csee.wvu.edu/~xinl/source.html\" target=\"_blank\">http://www.csee.wvu.edu/~xinl/source.html</a>&nbsp;深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合</p>\n<p>（196）西班牙格拉纳达大学超分辨率重建项目组：<a href=\"http://decsai.ugr.es/pi/superresolution/index.html\" target=\"_blank\">http://decsai.ugr.es/pi/superresolution/index.html</a></p>\n<p>（197）清华大学程明明博士：<a href=\"http://mmcheng.net/\" target=\"_blank\">http://mmcheng.net/</a>&nbsp;图像分割、检索</p>\n<p>（198）牛津布鲁克斯大学Philip H.S.Torr教授：<a href=\"http://cms.brookes.ac.uk/staff/PhilipTorr/\" target=\"_blank\">http://cms.brookes.ac.uk/staff/PhilipTorr/</a>&nbsp;分割、三维重建</p>\n<p>（199）佐治亚理工学院James M.Rehg教授：<a href=\"http://www.cc.gatech.edu/~rehg/\" target=\"_blank\">http://www.cc.gatech.edu/~rehg/</a>&nbsp;分割、行人检测、特征描述、</p>\n<p>（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:</p>\n<p>&nbsp;<a href=\"http://www.image-net.org/challenges/LSVRC/2013/\" target=\"_blank\">http://www.image-net.org/challenges/LSVRC/2013/</a></p>\n<p>（201）加州大学尔湾分校Deva Ramanan助理教授：<a href=\"http://www.ics.uci.edu/~dramanan/\" target=\"_blank\">http://www.ics.uci.edu/~dramanan/</a>&nbsp;目标检测，行人检测，跟踪、稀疏表示</p>\n<p>（202）人脸识别测试图片集：<a href=\"http://www.mlcv.net/\" target=\"_blank\">http://www.mlcv.net/</a></p>\n<p>（203）美国西北大学博士Ming Yang:&nbsp;http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；</p>\n<p>（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）</p>\n<p>（205）中文语言资源联盟：http://www.chineseldc.org/index.html &nbsp;内有很多语言识别、字符识别的训练，测试库；</p>\n<p>（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶</p>\n<p>（207）德国戴姆勒研究所<span>Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、</span></p>\n<p>（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪</p>\n<p>（209）Libqrencode:&nbsp;http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库</p>\n<p>（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html# &nbsp;烟雾检测、3D重建、医学图像处理</p>\n<p>（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/ &nbsp;图像增强、</p>\n<p>（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强</p>\n<p>3 代码汇总</p>\n<p>&nbsp;</p>\n<p>一、特征提取Feature Extraction：</p>\n<ul>\n<li>\n<p>SIFT [1] [<a href=\"http://www.cs.ubc.ca/~lowe/keypoints/siftDemoV4.zip\" target=\"_blank\">Demo program</a>][<a href=\"http://blogs.oregonstate.edu/hess/code/sift/\" target=\"_blank\">SIFT Library</a>] [<a href=\"http://www.vlfeat.org/\" target=\"_blank\">VLFeat</a>]</p>\n</li>\n<li>\n<p>PCA-SIFT [2] [<a href=\"http://www.cs.cmu.edu/~yke/pcasift/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Affine-SIFT [3] [<a href=\"http://www.ipol.im/pub/algo/my_affine_sift/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>SURF [4] [<a href=\"http://www.chrisevansdev.com/computer-vision-opensurf.html\" target=\"_blank\">OpenSURF</a>] [<a href=\"http://www.maths.lth.se/matematiklth/personal/petter/surfmex.php\" target=\"_blank\">Matlab Wrapper</a>]</p>\n</li>\n<li>\n<p>Affine Covariant Features [5] [<a href=\"http://www.robots.ox.ac.uk/~vgg/research/affine/\" target=\"_blank\">Oxford project</a>]</p>\n</li>\n<li>\n<p>MSER [6] [<a href=\"http://www.robots.ox.ac.uk/~vgg/research/affine/\" target=\"_blank\">Oxford project</a>] [<a href=\"http://www.vlfeat.org/\" target=\"_blank\">VLFeat</a>]</p>\n</li>\n<li>\n<p>Geometric Blur [7] [<a href=\"http://www.robots.ox.ac.uk/~vgg/software/MKL/\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Local Self-Similarity Descriptor [8] [<a href=\"http://www.robots.ox.ac.uk/~vgg/software/SelfSimilarity/\" target=\"_blank\">Oxford implementation</a>]</p>\n</li>\n<li>\n<p>Global and Efficient Self-Similarity [9] [<a href=\"http://www.vision.ee.ethz.ch/~calvin/gss/selfsim_release1.0.tgz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Histogram of Oriented Graidents [10] [<a href=\"http://www.navneetdalal.com/software\" target=\"_blank\">INRIA Object Localization Toolkit</a>] [<a href=\"http://www.computing.edu.au/~12482661/hog.html\" target=\"_blank\">OLT toolkit for Windows</a>]</p>\n</li>\n<li>\n<p>GIST [11] [<a href=\"http://people.csail.mit.edu/torralba/code/spatialenvelope/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Shape Context [12] [<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sc_digits.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Color Descriptor [13] [<a href=\"http://koen.me/research/colordescriptors/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Pyramids of Histograms of Oriented Gradients [<a href=\"http://www.robots.ox.ac.uk/~vgg/research/caltech/phog/phog.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Space-Time Interest Points (STIP) [14][<a href=\"http://www.nada.kth.se/cvap/abstracts/cvap284.html\" target=\"_blank\">Project</a>] [<a href=\"http://www.irisa.fr/vista/Equipe/People/Laptev/download/stip-1.1-winlinux.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Boundary Preserving Dense Local Regions [15][<a href=\"http://vision.cs.utexas.edu/projects/bplr/bplr.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Weighted Histogram[<a href=\"http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/whistc.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Histogram-based Interest Points Detectors[<a href=\"http://www.cs.nthu.edu.tw/~htchen/hipd_cvpr09.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://740-2.cs.nthu.edu.tw/~htchen/hipd/hist_corner.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>An OpenCV - C++ implementation of Local Self Similarity Descriptors [<a href=\"http://intuitionlogic.com/post/2011/04/11/A-OpenCV-C++-implementation-of-Local-Self-Similarity-Descriptors.aspx\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Fast Sparse Representation with Prototypes[<a href=\"http://faculty.ucmerced.edu/mhyang/cvpr10_fsr.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Corner Detection [<a href=\"http://kiwi.cs.dal.ca/~dparks/CornerDetection/index.htm\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>AGAST Corner Detector: faster than FAST and even FAST-ER[<a href=\"http://www6.in.tum.de/Main/ResearchAgast\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Real-time Facial Feature Detection using Conditional Regression Forests[<a href=\"http://files.is.tue.mpg.de/jgall/projects/facialfeatures/facialfeatures.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Global and Efficient Self-Similarity for Object Classification and Detection[<a href=\"http://groups.inf.ed.ac.uk/calvin/gss/selfsim_release1.0.tgz\" target=\"_blank\">code</a>]</p>\n</li>\n<li>\n<p>WαSH: Weighted α-Shapes for Local Feature Detection[<a href=\"http://image.ntua.gr/iva/research/wash/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>HOG[<a href=\"http://soc.fudan.edu.cn/vip/projects/gradproj/wiki/HOG%E4%BB%A3%E7%A0%81\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Online Selection of Discriminative Tracking Features[<a href=\"http://www.cs.ucla.edu/~roozbehm/cs7495/report.html\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>二、图像分割Image Segmentation：</p>\n<ul>\n<li>\n<p>Normalized Cut [1] [<a href=\"http://www.cis.upenn.edu/~jshi/software/\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Gerg Mori’ Superpixel code [2] [<a href=\"http://www.cs.sfu.ca/~mori/research/superpixels/\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Efficient Graph-based Image Segmentation [3] [<a href=\"http://people.cs.uchicago.edu/~pff/segment/\" target=\"_blank\">C++ code</a>] [<a href=\"http://www.mathworks.com/matlabcentral/fileexchange/25866-efficient-graph-based-image-segmentation\" target=\"_blank\">Matlab wrapper</a>]</p>\n</li>\n<li>\n<p>Mean-Shift Image Segmentation [4] [<a href=\"http://coewww.rutgers.edu/riul/research/code/EDISON/index.html\" target=\"_blank\">EDISON C++ code</a>] [<a href=\"http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/edison_matlab_interface.tar.gz\" target=\"_blank\">Matlab wrapper</a>]</p>\n</li>\n<li>\n<p>OWT-UCM Hierarchical Segmentation [5] [<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html\" target=\"_blank\">Resources</a>]</p>\n</li>\n<li>\n<p>Turbepixels [6] [<a href=\"http://www.cs.toronto.edu/~babalex/turbopixels_code.tar.gz\" target=\"_blank\">Matlab code 32bit</a>] [<a href=\"http://www.cs.toronto.edu/~babalex/TurboPixels64.rar\" target=\"_blank\">Matlab code 64bit</a>] [<a href=\"http://www.cs.toronto.edu/~babalex/superpixels_update.tgz\" target=\"_blank\">Updated code</a>]</p>\n</li>\n<li>\n<p>Quick-Shift [7] [<a href=\"http://www.vlfeat.org/overview/quickshift.html\" target=\"_blank\">VLFeat</a>]</p>\n</li>\n<li>\n<p>SLIC Superpixels [8] [<a href=\"http://ivrgwww.epfl.ch/supplementary_material/RK_SLICSuperpixels/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Segmentation by Minimum Code Length [9] [<a href=\"http://perception.csl.uiuc.edu/coding/image_segmentation/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Biased Normalized Cut [10] [<a href=\"http://www.cs.berkeley.edu/~smaji/projects/biasedNcuts/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Segmentation Tree [11-12] [<a href=\"http://vision.ai.uiuc.edu/segmentation\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Entropy Rate Superpixel Segmentation [13] [<a href=\"http://www.umiacs.umd.edu/~mingyliu/src/ers_matlab_wrapper_v0.1.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Fast Approximate Energy Minimization via Graph Cuts[<a href=\"http://www.csd.uwo.ca/faculty/olga/Papers/pami01_final.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://vision.csd.uwo.ca/code/gco-v3.0.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Efﬁcient Planar Graph Cuts with Applications in Computer Vision[<a href=\"http://www.csd.uwo.ca/~schmidtf/pdf/schmidt_et_al_cvpr09.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://vision.csd.uwo.ca/code/PlanarCut-v1.0.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Isoperimetric Graph Partitioning for Image Segmentation[<a href=\"http://www.cns.bu.edu/~lgrady/grady2006isoperimetric.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://www.cns.bu.edu/~lgrady/grady2006isoperimetric_code.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Random Walks for Image Segmentation[<a href=\"http://www.cns.bu.edu/~lgrady/grady2006random.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://www.cns.bu.edu/~lgrady/random_walker_matlab_code.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Blossom V: A new implementation of a minimum cost perfect matching algorithm[<a href=\"http://pub.ist.ac.at/~vnk/software/blossom5-v2.03.src.tar.gz%20%20http:/pub.ist.ac.at/~vnk/software/blossom5-v2.03.src.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[<a href=\"http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://pub.ist.ac.at/~vnk/software/maxflow-v3.01.src.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Geodesic Star Convexity for Interactive Image Segmentation[<a href=\"http://www.robots.ox.ac.uk/~vgg/software/iseg/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Contour Detection and Image Segmentation Resources[<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html\" target=\"_blank\">Project</a>][<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_source.tgz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Biased Normalized Cuts[<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/biasedNcuts/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Max-flow/min-cut[<a href=\"http://vision.csd.uwo.ca/code/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Chan-Vese Segmentation using Level Set[<a href=\"http://www.ipol.im/pub/art/2012/g-cv/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>A Toolbox of Level Set Methods[<a href=\"http://www.cs.ubc.ca/~mitchell/ToolboxLS/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Re-initialization Free Level Set Evolution via Reaction Diffusion[<a href=\"http://www4.comp.polyu.edu.hk/~cslzhang/RD/RD.htm\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Improved C-V active contour model[<a href=\"http://www4.comp.polyu.edu.hk/~cskhzhang/J_papers/ICV.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://www4.comp.polyu.edu.hk/~cskhzhang/J_papers/ICV.rar\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[<a href=\"http://www4.comp.polyu.edu.hk/~cskhzhang/J_papers/ICIP10_SVMLS.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://www4.comp.polyu.edu.hk/~cskhzhang/J_papers/SVMLS_v0.rar\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Level Set Method Research by Chunming Li[<a href=\"http://www.engr.uconn.edu/~cmli/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>ClassCut for Unsupervised Class Segmentation[<a href=\"http://groups.inf.ed.ac.uk/calvin/classcut/ClassCut-release_v1.0.zip\" target=\"_blank\">cod</a>e]</p>\n</li>\n<li>\n<p>SEEDS: Superpixels Extracted via Energy-Driven Sampling&nbsp;<a href=\"http://www.vision.ee.ethz.ch/~vamichae/seeds/\" target=\"_blank\">[Project</a>][<a href=\"http://www.mvdblive.org/seeds/\" target=\"_blank\">other</a>]</p>\n</li>\n</ul>\n<br>\n<p>三、目标检测Object Detection：</p>\n<ul>\n<li>\n<p>A simple object detector with boosting [<a href=\"http://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>INRIA Object Detection and Localization Toolkit [1] [<a href=\"http://pascal.inrialpes.fr/soft/olt/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Discriminatively Trained Deformable Part Models [2] [<a href=\"http://people.cs.uchicago.edu/~pff/latent/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Cascade Object Detection with Deformable Part Models [3] [<a href=\"http://people.cs.uchicago.edu/~rbg/star-cascade/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Poselet [4] [<a href=\"http://www.eecs.berkeley.edu/~lbourdev/poselets/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Implicit Shape Model [5] [<a href=\"http://www.vision.ee.ethz.ch/~bleibe/code/ism.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Viola and Jones’s Face Detection [6] [<a href=\"http://pr.willowgarage.com/wiki/Face_detection\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Bayesian Modelling of Dyanmic Scenes for Object Detection[<a href=\"http://vision.eecs.ucf.edu/papers/01512057.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://vision.eecs.ucf.edu/Code/Background.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Hand detection using multiple proposals[<a href=\"http://www.robots.ox.ac.uk/~vgg/software/hands/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Color Constancy, Intrinsic Images, and Shape Estimation[<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/reconstruction/BarronMalikECCV2012.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://www.cs.berkeley.edu/~barron/BarronMalikECCV2012_code.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Discriminatively trained deformable part models[<a href=\"http://people.cs.uchicago.edu/~rbg/latent/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [<a href=\"http://campar.cs.tum.edu/Main/StefanHinterstoisser\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Image Processing On Line[<a href=\"http://www.ipol.im/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Robust Optical Flow Estimation[<a href=\"http://www.ipol.im/pub/pre/21/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Where's Waldo: Matching People in Images of Crowds[<a href=\"http://homes.cs.washington.edu/~rahul/data/WheresWaldo.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Scalable Multi-class Object Detection[<a href=\"http://files.is.tue.mpg.de/jgall/projects/houghMC/houghMC.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Class-Specific Hough Forests for Object Detection[<a href=\"http://files.is.tue.mpg.de/jgall/projects/houghforest/houghforest.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Deformed Lattice Detection In Real-World Images[<a href=\"http://vision.cse.psu.edu/data/data.shtml\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Discriminatively trained deformable part models[<a href=\"http://people.cs.uchicago.edu/~rbg/latent/\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>四、显著性检测Saliency Detection：</p>\n<ul>\n<li>\n<p>Itti, Koch, and Niebur’ saliency detection [1] [<a href=\"http://www.saliencytoolbox.net/\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Frequency-tuned salient region detection [2] [<a href=\"http://ivrgwww.epfl.ch/supplementary_material/RK_CVPR09/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Saliency detection using maximum symmetric surround [3] [<a href=\"http://ivrg.epfl.ch/supplementary_material/RK_ICIP2010/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Attention via Information Maximization [4] [<a href=\"http://www.cse.yorku.ca/~neil/AIM.zip\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Context-aware saliency detection [5] [<a href=\"http://webee.technion.ac.il/labs/cgm/Computer-Graphics-Multimedia/Software/Saliency/Saliency.html\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Graph-based visual saliency [6] [<a href=\"http://www.klab.caltech.edu/~harel/share/gbvs.php\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Saliency detection: A spectral residual approach. [7] [<a href=\"http://www.klab.caltech.edu/~xhou/projects/spectralResidual/spectralresidual.html\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Segmenting salient objects from images and videos. [8] [<a href=\"http://www.cse.oulu.fi/MVG/Downloads/saliency\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Saliency Using Natural statistics. [9] [<a href=\"http://cseweb.ucsd.edu/~l6zhang/\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [<a href=\"http://www.svcl.ucsd.edu/projects/saliency/\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Learning to Predict Where Humans Look [11] [<a href=\"http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Global Contrast based Salient Region Detection [12] [<a href=\"http://cg.cs.tsinghua.edu.cn/people/~cmm/saliency/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Bayesian Saliency via Low and Mid Level Cues[<a href=\"http://ice.dlut.edu.cn/lu/Project/TIP_scm/TIP_scm.htm\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Top-Down Visual Saliency via Joint CRF and Dictionary Learning[<a href=\"http://faculty.ucmerced.edu/mhyang/papers/cvpr12a.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://faculty.ucmerced.edu/mhyang/code/top-down-saliency.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Saliency Detection: A Spectral Residual Approach[<a href=\"http://www.klab.caltech.edu/~xhou/projects/dva/dva.html\" target=\"_blank\">Code</a>]</p>\n</li>\n</ul>\n<br>\n<p>五、图像分类、聚类Image Classification, Clustering</p>\n<ul>\n<li>\n<p>Pyramid Match [1] [<a href=\"http://people.csail.mit.edu/jjl/libpmk/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Spatial Pyramid Matching [2] [<a href=\"http://www.cs.unc.edu/~lazebnik/research/SpatialPyramid.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Locality-constrained Linear Coding [3] [<a href=\"http://www.ifp.illinois.edu/~jyang29/LLC.htm\" target=\"_blank\">Project</a>] [<a href=\"http://www.ifp.illinois.edu/~jyang29/codes/CVPR10-LLC.rar\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Sparse Coding [4] [<a href=\"http://www.ifp.illinois.edu/~jyang29/ScSPM.htm\" target=\"_blank\">Project</a>] [<a href=\"http://www.ifp.illinois.edu/~jyang29/codes/CVPR09-ScSPM.rar\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Texture Classification [5] [<a href=\"http://www.robots.ox.ac.uk/~vgg/research/texclass/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Multiple Kernels for Image Classification [6] [<a href=\"http://www.robots.ox.ac.uk/~vgg/software/MKL/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Feature Combination [7] [<a href=\"http://www.vision.ee.ethz.ch/~pgehler/projects/iccv09/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>SuperParsing [<a href=\"http://www.cs.unc.edu/~jtighe/Papers/ECCV10/eccv10-jtighe-code.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Large Scale Correlation Clustering Optimization[<a href=\"http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/LargeScaleCC1.0.tar.gz\" target=\"_blank\">Matlab code</a>]</p>\n</li>\n<li>\n<p>Detecting and Sketching the Common[<a href=\"http://www.wisdom.weizmann.ac.il/~vision/SketchTheCommon\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Self-Tuning Spectral Clustering[<a href=\"http://www.vision.caltech.edu/lihi/Demos/SelfTuningClustering.html\" target=\"_blank\">Project</a>][<a href=\"http://www.vision.caltech.edu/lihi/Demos/SelfTuning/ZPclustering.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[<a href=\"http://www.wisdom.weizmann.ac.il/~levina/papers/assisted-eccv04.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://www.wisdom.weizmann.ac.il/~levina/papers/reflections.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Filters for Texture Classification[<a href=\"http://www.robots.ox.ac.uk/~vgg/research/texclass/filters.html#download\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Multiple Kernel Learning for Image Classification[<a href=\"http://www.robots.ox.ac.uk/~vgg/software/MKL/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>SLIC Superpixels[<a href=\"http://ivrg.epfl.ch/supplementary_material/RK_SLICSuperpixels/\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>六、抠图Image Matting</p>\n<ul>\n<li>\n<p>A Closed Form Solution to Natural Image Matting [<a href=\"http://people.csail.mit.edu/alevin/matting.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Spectral Matting [<a href=\"http://www.vision.huji.ac.il/SpectralMatting/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Learning-based Matting [<a href=\"http://www.mathworks.com/matlabcentral/fileexchange/31412\" target=\"_blank\">Code</a>]</p>\n</li>\n</ul>\n<br>\n<p>七、目标跟踪Object Tracking：</p>\n<ul>\n<li>\n<p>A Forest of Sensors - Tracking Adaptive Background Mixture Models [<a href=\"http://www.ai.mit.edu/projects/vsam/Tracking/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Object Tracking via Partial Least Squares Analysis[<a href=\"http://faculty.ucmerced.edu/mhyang/papers/tip12_pls_tracking.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://faculty.ucmerced.edu/mhyang/code/PLS_tracker_tip.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Robust Object Tracking with Online Multiple Instance Learning[<a href=\"http://faculty.ucmerced.edu/mhyang/papers/pami11b.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Online Visual Tracking with Histograms and Articulating Blocks[<a href=\"http://www.cise.ufl.edu/~smshahed/tracking.htm\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Incremental Learning for Robust Visual Tracking[<a href=\"http://www.cs.toronto.edu/~dross/ivt/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Real-time Compressive Tracking[<a href=\"http://www4.comp.polyu.edu.hk/~cslzhang/CT/CT.htm\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Robust Object Tracking via Sparsity-based Collaborative Model[<a href=\"http://faculty.ucmerced.edu/mhyang/project/cvpr12_scm.htm\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Visual Tracking via Adaptive Structural Local Sparse Appearance Model[<a href=\"http://faculty.ucmerced.edu/mhyang/project/cvpr12_jia_project.htm\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Online Discriminative Object Tracking with Local Sparse Representation[<a href=\"http://faculty.ucmerced.edu/mhyang/papers/wacv12a.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://faculty.ucmerced.edu/mhyang/code/wacv12a_code.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Superpixel Tracking[<a href=\"http://faculty.ucmerced.edu/mhyang/papers/iccv11a.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[<a href=\"http://faculty.ucmerced.edu/mhyang/papers/bmvc11a.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://faculty.ucmerced.edu/mhyang/code/BMVC11-HSSL-package.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Online Multiple Support Instance Tracking [<a href=\"http://faculty.ucmerced.edu/mhyang/papers/fg11a.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://faculty.ucmerced.edu/mhyang/code/fg11_omsit.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Visual Tracking with Online Multiple Instance Learning[<a href=\"http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Object detection and recognition[<a href=\"http://c2inet.sce.ntu.edu.sg/Jianxin/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Compressive Sensing Resources[<a href=\"http://dsp.rice.edu/cs\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[<a href=\"http://www.robots.ox.ac.uk/~cbibby/index.shtml\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Tracking-Learning-Detection[<a href=\"http://info.ee.surrey.ac.uk/Personal/Z.Kalal/\" target=\"_blank\">Project</a>][<a href=\"https://github.com/arthurv/OpenTLD\" target=\"_blank\">OpenTLD/C++ Code</a>]</p>\n</li>\n<li>\n<p>the HandVu：vision-based hand gesture interface[<a href=\"http://ilab.cs.ucsb.edu/index.php/component/content/article/12/29\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[<a href=\"http://files.is.tue.mpg.de/jgall/projects/stochGPLVM/stochGPLVM.html\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>八、Kinect：</p>\n<ul>\n<li>\n<p>Kinect toolbox[<a href=\"http://kinecttoolbox.codeplex.com/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>OpenNI[<a href=\"http://www.openni.org/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>zouxy09 CSDN Blog[<a href=\"http://blog.csdn.net/zouxy09/article/details/8145688\" target=\"_blank\">Resource</a>]</p>\n</li>\n<li>\n<p>FingerTracker 手指跟踪[<a href=\"http://makematics.com/code/FingerTracker/\" target=\"_blank\">code</a>]</p>\n</li>\n</ul>\n<br>\n<p>九、3D相关：</p>\n<ul>\n<li>\n<p>3D Reconstruction of a Moving Object[<a href=\"http://www.wisdom.weizmann.ac.il/~ronen/papers/Simakov%20Frolova%20Basri%20-%20Dense%20Shape%20Reconstruction%20Under%20Arbitrary%20Unknown%20Lighting.pdf\" target=\"_blank\">Paper</a>] [<a href=\"http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/SFB_matlab1.0.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Shape From Shading Using Linear Approximation[<a href=\"http://vision.eecs.ucf.edu/shadsrc.html\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Combining Shape from Shading and Stereo Depth Maps[<a href=\"http://vision.eecs.ucf.edu/combsrc.html\" target=\"_blank\">Project</a>][<a href=\"http://vision.eecs.ucf.edu/projects/ShapeFromShading/combine.tar.Z\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Shape from Shading: A Survey[<a href=\"http://vision.eecs.ucf.edu/papers/shah/99/ZTCS99.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://vision.eecs.ucf.edu/projects/ShapeFromShading/SFS_Survey_1_00.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>A Spatio-Temporal Descriptor based on 3D Gradients [HOG3D][<a href=\"http://lear.inrialpes.fr/people/klaeser/research_hog3d\" target=\"_blank\">Project</a>](<a href=\"http://lear.inrialpes.fr/people/klaeser/software_3d_video_descriptor\" target=\"_blank\">Code</a>)</p>\n</li>\n<li>\n<p>Multi-camera Scene Reconstruction via Graph Cuts[<a href=\"http://www.cs.cornell.edu/~rdz/papers/kz-eccv02-recon.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://pub.ist.ac.at/~vnk/software/match-v3.4.src.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[<a href=\"http://www.cs.ucf.edu/~vision\" target=\"_blank\">Paper</a>][<a href=\"http://www.ee.cityu.edu.hk/~syyuen/Public/SfS/PRL_Perspective_FMM.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/reconstruction/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[<a href=\"http://faculty.ucmerced.edu/mhyang/code/PackagedTrackingCode.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Learning 3-D Scene Structure from a Single Still Image[<a href=\"http://ai.stanford.edu/~asaxena/reconstruction3d/\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>十、机器学习算法：</p>\n<ul>\n<li>\n<p>Matlab class for computing Approximate Nearest Nieghbor (ANN) [<a href=\"http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/ann_wrapper_Mar2012.tar.gz\" target=\"_blank\">Matlab class</a>&nbsp;providing interface to<a href=\"http://www.cs.umd.edu/~mount/ANN/\" target=\"_blank\">ANN library</a>]</p>\n</li>\n<li>\n<p>Random Sampling[<a href=\"http://www.wisdom.weizmann.ac.il/~bagon/matlab_code/weight_sample.tar.gz\" target=\"_blank\">code</a>]</p>\n</li>\n<li>\n<p>Probabilistic Latent Semantic Analysis [pLSA](<a href=\"http://www.robots.ox.ac.uk/~vgg/software/pLSA/pLSA_demo.tgz\" target=\"_blank\">Code</a>)</p>\n</li>\n<li>\n<p>FASTANN and FASTCLUSTER for approximate k-means [AKM](<a href=\"http://www.robots.ox.ac.uk/~vgg/software/fastann/\" target=\"_blank\">Project</a>)</p>\n</li>\n<li>\n<p>Fast Intersection / Additive Kernel SVMs[<a href=\"http://www.cs.berkeley.edu/~smaji/projects/fiksvm/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>SVM[<a href=\"http://osmot.cs.cornell.edu/svm_light/\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Ensemble learning[<a href=\"http://c2inet.sce.ntu.edu.sg/Jianxin/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Deep Learning[<a href=\"http://deeplearning.net/\" target=\"_blank\">Net</a>]</p>\n</li>\n<li>\n<p>Deep Learning Methods for Vision[<a href=\"http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Neural Network for Recognition of Handwritten Digits[<a href=\"http://www.codeproject.com/KB/library/NeuralNetRecognition.aspx\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Training a deep autoencoder or a classifier on MNIST digits[<a href=\"http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>THE MNIST DATABASE of handwritten digits[<a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Ersatz：deep neural networks in the cloud[<a href=\"http://www.ersatz1.com/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Deep Learning [<a href=\"http://www.cs.nyu.edu/~yann/research/deep/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[<a href=\"http://www.ics.forth.gr/~lourakis/sparseLM/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Weka 3: Data Mining Software in Java[<a href=\"http://www.cs.waikato.ac.nz/ml/weka/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu [余凯](<a href=\"http://vipl.ict.ac.cn/News/academic-report-tutorial-deep-learning-dr-kai-yu\" target=\"_blank\">Video</a>)</p>\n</li>\n<li>\n<p>CNN - Convolutional neural network class[<a href=\"http://www.mathworks.cn/matlabcentral/fileexchange/24291\" target=\"_blank\">Matlab Tool</a>]</p>\n</li>\n<li>\n<p>Yann LeCun's Publications[<a href=\"http://yann.lecun.com/exdb/publis/index.html#lecun-98\" target=\"_blank\">Wedsite</a>]</p>\n</li>\n<li>\n<p>LeNet-5, convolutional neural networks[<a href=\"http://yann.lecun.com/exdb/lenet/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Training a deep autoencoder or a classifier on MNIST digits[<a href=\"http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Deep Learning 大牛Geoffrey E. Hinton's HomePage[<a href=\"http://www.cs.toronto.edu/~hinton/\" target=\"_blank\">Website</a>]</p>\n</li>\n<li>\n<p>Multiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning [LDML](<a href=\"http://lear.inrialpes.fr/people/guillaumin/code.php#mildml\" target=\"_blank\">Code</a>)</p>\n</li>\n<li>\n<p>Sparse coding simulation software[<a href=\"http://redwood.berkeley.edu/bruno/sparsenet/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Visual Recognition and Machine Learning Summer School[<a href=\"http://lear.inrialpes.fr/software\" target=\"_blank\">Software</a>]</p>\n</li>\n</ul>\n<br>\n<p>十一、目标、行为识别Object, Action Recognition：</p>\n<ul>\n<li>\n<p>Action Recognition by Dense Trajectories[<a href=\"http://lear.inrialpes.fr/people/wang/dense_trajectories\" target=\"_blank\">Project</a>][<a href=\"http://lear.inrialpes.fr/people/wang/download/dense_trajectory_release.tar.gz\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Action Recognition Using a Distributed Representation of Pose and Appearance[<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/action/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Recognition Using Regions[<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/glam-cvpr09.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/glam_cvpr09_v2.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>2D Articulated Human Pose Estimation[<a href=\"http://www.vision.ee.ethz.ch/~calvin/articulated_human_pose_estimation_code/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[<a href=\"http://faculty.ucmerced.edu/mhyang/papers/cvpr07a.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://www.cise.ufl.edu/~smshahed/cvpr07_fast_human_pose.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Estimating Human Pose from Occluded Images[<a href=\"http://faculty.ucmerced.edu/mhyang/papers/accv09a.pdf\" target=\"_blank\">Paper</a>][<a href=\"http://faculty.ucmerced.edu/mhyang/code/accv09_pose.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Quasi-dense wide baseline matching[<a href=\"http://www.ee.oulu.fi/~jkannala/quasidense/quasidense.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[<a href=\"http://gesture.chalearn.org/data/sample-code\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Real Time Head Pose Estimation with Random Regression Forests[<a href=\"http://files.is.tue.mpg.de/jgall/projects/RFhead/RFhead.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>2D Action Recognition Serves 3D Human Pose Estimation[</p>\n</li>\n<li>\n<p>A Hough Transform-Based Voting Framework for Action Recognition[</p>\n</li>\n<li>\n<p>Motion Interchange Patterns for Action Recognition in Unconstrained Videos[</p>\n</li>\n<li>\n<p>2D articulated human pose estimation software[<a href=\"http://groups.inf.ed.ac.uk/calvin/articulated_human_pose_estimation_code/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Learning and detecting shape models [<a href=\"http://groups.inf.ed.ac.uk/calvin/release-learn-shapes-v1.3.tgz\" target=\"_blank\">code</a>]</p>\n</li>\n<li>\n<p>Progressive Search Space Reduction for Human Pose Estimation[<a href=\"http://www.robots.ox.ac.uk/~vgg/software/UpperBody/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Learning Non-Rigid 3D Shape from 2D Motion[<a href=\"http://movement.stanford.edu/learning-nr-shape/\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>十二、图像处理：</p>\n<ul>\n<li>\n<p>Distance Transforms of Sampled Functions[<a href=\"http://cs.brown.edu/~pff/dt/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>The Computer Vision Homepage[<a href=\"http://www.cs.cmu.edu/~cil/vision.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Efficient appearance distances between windows[<a href=\"http://groups.inf.ed.ac.uk/calvin/efficientAppDistances/releaseEfficientAppDistances.zip\" target=\"_blank\">code</a>]</p>\n</li>\n<li>\n<p>Image Exploration algorithm[<a href=\"http://groups.inf.ed.ac.uk/calvin/ReleasedCode/image_exploration_v1.1.tgz\" target=\"_blank\">code</a>]</p>\n</li>\n<li>\n<p>Motion Magnification 运动放大 [<a href=\"http://people.csail.mit.edu/celiu/motionmag/motionmag.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Bilateral Filtering for Gray and Color Images 双边滤波器 [<a href=\"http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>A Fast Approximation of the Bilateral Filter using a Signal Processing Approach [</p>\n</li>\n</ul>\n<br>\n<p>十三、一些实用工具：</p>\n<ul>\n<li>\n<p>EGT: a Toolbox for Multiple View Geometry and Visual Servoing[<a href=\"http://egt.dii.unisi.it/\" target=\"_blank\">Project</a>] [<a href=\"http://egt.dii.unisi.it/download/EGT_v1p3.zip\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>a development kit of matlab mex functions for OpenCV library[<a href=\"http://www.cs.stonybrook.edu/~kyamagu/mexopencv/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Fast Artificial Neural Network Library[<a href=\"http://leenissen.dk/fann/wp/\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>十四、人手及指尖检测与识别：</p>\n<ul>\n<li>\n<p>finger-detection-and-gesture-recognition [<a href=\"http://code.google.com/p/finger-detection-and-gesture-recognition/downloads/list\" target=\"_blank\">Code</a>]</p>\n</li>\n<li>\n<p>Hand and Finger Detection using JavaCV[<a href=\"http://www.javacodegeeks.com/2012/12/hand-and-finger-detection-using-javacv.html?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+JavaCodeGeeks+%28Java+Code+Geeks%29\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Hand and fingers detection[<a href=\"http://forum.openframeworks.cc/index.php?topic=1916.0\" target=\"_blank\">Code</a>]</p>\n</li>\n</ul>\n<br>\n<p>十五、场景解释：</p>\n<ul>\n<li>\n<p>Nonparametric Scene Parsing via Label Transfer [<a href=\"http://people.csail.mit.edu/celiu/LabelTransfer/code.html\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>十六、光流Optical flow：</p>\n<ul>\n<li>\n<p>High accuracy optical flow using a theory for warping [<a href=\"http://perception.inrialpes.fr/~chari/myweb/Software/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Dense Trajectories Video Description [<a href=\"http://lear.inrialpes.fr/people/wang/dense_trajectories\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>SIFT Flow: Dense Correspondence across Scenes and its Applications[<a href=\"http://people.csail.mit.edu/celiu/SIFTflow/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>KLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [<a href=\"http://www.ces.clemson.edu/~stb/klt/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Tracking Cars Using Optical Flow[<a href=\"http://www.mathworks.cn/cn/help/vision/examples/tracking-cars-using-optical-flow.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Secrets of optical flow estimation and their principles[<a href=\"http://ps.is.tue.mpg.de/person/black#tabs-code\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>implmentation of the Black and Anandan dense optical flow method[<a href=\"http://ps.is.tue.mpg.de/person/black#tabs-code\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Optical Flow Computation[<a href=\"https://www.ceremade.dauphine.fr/~peyre/numerical-tour/tours/multidim_5_opticalflow/#37\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Beyond Pixels: Exploring New Representations and Applications for Motion Analysis[<a href=\"http://people.csail.mit.edu/celiu/OpticalFlow/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>A Database and Evaluation Methodology for Optical Flow[<a href=\"http://vision.middlebury.edu/flow/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>optical flow relative[<a href=\"http://lmb.informatik.uni-freiburg.de/resources/software.php\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Robust Optical Flow Estimation [<a href=\"http://www.ipol.im/pub/pre/21/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>optical flow[<a href=\"http://www.jonathanmugan.com/GraphicsProject/OpticalFlow/\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>十七、图像检索Image Retrieval：</p>\n<ul>\n<li>\n<p>Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval&nbsp;<a href=\"http://www.ee.columbia.edu/~wliu/CVPR08_ssml.pdf\" target=\"_blank\">[Paper</a>][<a href=\"http://www.ee.columbia.edu/~wliu/SSMetric.zip\" target=\"_blank\">code</a>]</p>\n</li>\n</ul>\n<br>\n<p>十八、马尔科夫随机场Markov Random Fields：</p>\n<ul>\n<li>\n<p>Markov Random Fields for Super-Resolution&nbsp;<a href=\"http://www.ee.columbia.edu/~wliu/CVPR08_ssml.pdf\" target=\"_blank\">[</a><a href=\"http://people.csail.mit.edu/billf/project%20pages/sresCode/Markov%20Random%20Fields%20for%20Super-Resolution.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [<a href=\"http://vision.middlebury.edu/MRF/\" target=\"_blank\">Project</a>]</p>\n</li>\n</ul>\n<br>\n<p>十九、运动检测Motion detection：</p>\n<ul>\n<li>\n<p>Moving Object Extraction, Using Models or Analysis of Regions&nbsp;<a href=\"http://www.ee.columbia.edu/~wliu/CVPR08_ssml.pdf\" target=\"_blank\">[</a><a href=\"http://www.visionbib.com/bibliography/motion-i763.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Background Subtraction: Experiments and Improvements for ViBe [<a href=\"http://www2.ulg.ac.be/telecom/publi/publications/mvd/VanDroogenbroeck2012Background/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [<a href=\"http://www.na.icar.cnr.it/~maddalena.l/MODLab/SoftwareSOBS.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>changedetection.net: A new change detection benchmark dataset[<a href=\"http://www.changedetection.net/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>ViBe - a powerful technique for background detection and subtraction in video sequences[<a href=\"http://www2.ulg.ac.be/telecom/research/vibe/\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Background Subtraction Program[<a href=\"http://www.umiacs.umd.edu/~knkim/UMD-BGS/index.html\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Motion Detection Algorithms[<a href=\"http://www.codeproject.com/Articles/10248/Motion-Detection-Algorithms\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Stuttgart Artificial Background Subtraction Dataset[<a href=\"http://www.vis.uni-stuttgart.de/index.php?id=sabs\" target=\"_blank\">Project</a>]</p>\n</li>\n<li>\n<p>Object Detection, Motion Estimation, and Tracking[<a href=\"http://www.mathworks.cn/cn/help/vision/motion-analysis-and-tracking.html\" target=\"_blank\">Project</a>]</p>\n<p>&nbsp;</p>\n<p>Feature Detection and Description</p>\n<p>General Libraries:</p>\n<ul>\n<li>\n<p><a href=\"http://www.vlfeat.org/\" target=\"_blank\">VLFeat</a>&nbsp;– Implementation of various feature descriptors (including SIFT, HOG, and LBP) and covariant feature detectors (including DoG, Hessian, Harris Laplace, Hessian Laplace, Multiscale Hessian, Multiscale Harris). Easy-to-use Matlab interface. See&nbsp;<a href=\"https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxlY2N2MTJmZWF0dXJlc3xneDo3ZDllMzVhMDA4YzEzNmU2\" target=\"_blank\">Modern features: Software</a>&nbsp;– Slides providing a demonstration of VLFeat and also links to other software. Check also&nbsp;<a href=\"https://sites.google.com/site/eccv12features/\" target=\"_blank\">VLFeat hands-on session training</a></p>\n</li>\n<li>\n<p><a href=\"http://opencv.org/\" target=\"_blank\">OpenCV</a>&nbsp;– Various implementations of modern feature detectors and descriptors (SIFT, SURF, FAST, BRIEF, ORB, FREAK, etc.)</p>\n</li>\n</ul>\n<br>\n<p>Fast Keypoint Detectors for Real-time Applications:</p>\n<ul>\n<li>\n<p><a href=\"http://www.edwardrosten.com/work/fast.html\" target=\"_blank\">FAST</a>&nbsp;– High-speed corner detector implementation for a wide variety of platforms</p>\n</li>\n<li>\n<p><a href=\"http://www6.in.tum.de/Main/ResearchAgast\" target=\"_blank\">AGAST</a>&nbsp;– Even faster than the FAST corner detector. A multi-scale version of this method is used for the BRISK descriptor (ECCV 2010).</p>\n</li>\n</ul>\n<br>\n<p>Binary Descriptors for Real-Time Applications:</p>\n<ul>\n<li>\n<p><a href=\"http://cvlab.epfl.ch/software/brief/\" target=\"_blank\">BRIEF</a>&nbsp;– C++ code for a fast and accurate interest point descriptor (not invariant to rotations and scale) (ECCV 2010)</p>\n</li>\n<li>\n<p><a href=\"http://docs.opencv.org/modules/features2d/doc/feature_detection_and_description.html\" target=\"_blank\">ORB</a>&nbsp;– OpenCV implementation of the Oriented-Brief (ORB) descriptor (invariant to rotations, but not scale)</p>\n</li>\n<li>\n<p><a href=\"http://www.asl.ethz.ch/people/lestefan/personal/BRISK\" target=\"_blank\">BRISK</a>&nbsp;– Efficient Binary descriptor invariant to rotations and scale. It includes a Matlab mex interface. (ICCV 2011)</p>\n</li>\n<li>\n<p><a href=\"http://www.ivpe.com/freak.htm\" target=\"_blank\">FREAK</a>&nbsp;– Faster than BRISK (invariant to rotations and scale) (CVPR 2012)</p>\n</li>\n</ul>\n<br>\n<p>SIFT and SURF Implementations:</p>\n<ul>\n<li>\n<p>SIFT:&nbsp;<a href=\"http://www.vlfeat.org/\" target=\"_blank\">VLFeat</a>,&nbsp;<a href=\"http://docs.opencv.org/modules/nonfree/doc/feature_detection.html\" target=\"_blank\">OpenCV</a>,&nbsp;<a href=\"http://www.cs.ubc.ca/~lowe/keypoints/\" target=\"_blank\">Original code</a>&nbsp;by David Lowe,&nbsp;<a href=\"http://cs.unc.edu/~ccwu/siftgpu/\" target=\"_blank\">GPU implementation</a>,&nbsp;<a href=\"http://robwhess.github.com/opensift/\" target=\"_blank\">OpenSIFT</a></p>\n</li>\n<li>\n<p>SURF:&nbsp;<a href=\"http://www.vision.ee.ethz.ch/~surf/\" target=\"_blank\">Herbert Bay’s code</a>,&nbsp;<a href=\"http://docs.opencv.org/modules/nonfree/doc/feature_detection.html\" target=\"_blank\">OpenCV</a>,&nbsp;<a href=\"http://www.visual-experiments.com/demos/gpusurf/\" target=\"_blank\">GPU-SURF</a></p>\n</li>\n</ul>\n<br>\n<p>Other Local Feature Detectors and Descriptors:</p>\n<ul>\n<li>\n<p><a href=\"http://www.robots.ox.ac.uk/~vgg/research/affine/\" target=\"_blank\">VGG Affine Covariant features</a>&nbsp;– Oxford code for various affine covariant feature detectors and descriptors.</p>\n</li>\n<li>\n<p><a href=\"http://vision.ia.ac.cn/Students/wzh/publication/liop/index.html\" target=\"_blank\">LIOP descriptor</a>&nbsp;– Source code for the Local Intensity order Pattern (LIOP) descriptor (ICCV 2011).</p>\n</li>\n<li>\n<p><a href=\"http://www.cs.cornell.edu/projects/symfeat/\" target=\"_blank\">Local Symmetry Features</a>&nbsp;– Source code for matching of local symmetry features under large variations in lighting, age, and rendering style (CVPR 2012).</p>\n</li>\n</ul>\n<br>\n<p>Global Image Descriptors:</p>\n<ul>\n<li>\n<p><a href=\"http://people.csail.mit.edu/torralba/code/spatialenvelope/\" target=\"_blank\">GIST</a>&nbsp;– Matlab code for the GIST descriptor</p>\n</li>\n<li>\n<p><a href=\"https://sites.google.com/site/wujx2001/home\" target=\"_blank\">CENTRIST</a>&nbsp;– Global visual descriptor for scene categorization and object detection (PAMI 2011)</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>Feature Coding and Pooling</p>\n<ul>\n<li>\n<p><a href=\"http://www.robots.ox.ac.uk/~vgg/software/enceval_toolkit/\" target=\"_blank\">VGG Feature Encoding Toolkit</a>&nbsp;– Source code for various state-of-the-art feature encoding methods – including Standard hard encoding, Kernel codebook encoding, Locality-constrained linear encoding, and Fisher kernel encoding.</p>\n</li>\n<li>\n<p><a href=\"http://www.cs.illinois.edu/homes/slazebni/\" target=\"_blank\">Spatial Pyramid Matching</a>&nbsp;– Source code for feature pooling based on spatial pyramid matching (widely used for image classification)</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>Convolutional Nets and Deep Learning</p>\n<ul>\n<li>\n<p><a href=\"http://eblearn.sourceforge.net/\" target=\"_blank\">EBLearn</a>&nbsp;– C++ Library for Energy-Based Learning. It includes several demos and step-by-step instructions to train classifiers based on convolutional neural networks.</p>\n</li>\n<li>\n<p><a href=\"http://www.torch.ch/\" target=\"_blank\">Torch7</a>&nbsp;– Provides a matlab-like environment for state-of-the-art machine learning algorithms, including a fast implementation of convolutional neural networks.</p>\n</li>\n<li>\n<p><a href=\"http://deeplearning.net/software_links/\" target=\"_blank\">Deep Learning</a>&nbsp;- Various links for deep learning software.</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>Part-Based Models</p>\n<p>&nbsp;</p>\n<ul>\n<li>\n<p><a href=\"http://people.cs.uchicago.edu/~rbg/latent/\" target=\"_blank\">Deformable Part-based Detector</a>&nbsp;– Library provided by the authors of the original paper (state-of-the-art in PASCAL VOC detection task)</p>\n</li>\n<li>\n<p><a href=\"http://vision.mas.ecp.fr/Personnel/iasonas/dpms.html\" target=\"_blank\">Efficient Deformable Part-Based Detector</a>&nbsp;– Branch-and-Bound implementation for a deformable part-based detector.</p>\n</li>\n<li>\n<p><a href=\"http://www.idiap.ch/~cdubout/coding.html\" target=\"_blank\">Accelerated Deformable Part Model</a>&nbsp;– Efficient implementation of a method that achieves the exact same performance of deformable part-based detectors but with significant acceleration (ECCV 2012).</p>\n</li>\n<li>\n<p><a href=\"http://iselab.cvc.uab.es/CoarseToFine\" target=\"_blank\">Coarse-to-Fine Deformable Part Model</a>&nbsp;– Fast approach for deformable object detection (CVPR 2011).</p>\n</li>\n<li>\n<p><a href=\"http://www.eecs.berkeley.edu/~lbourdev/poselets/\" target=\"_blank\">Poselets</a>&nbsp;– C++ and Matlab versions for object detection based on poselets.</p>\n</li>\n<li>\n<p><a href=\"http://www.ics.uci.edu/~xzhu/face/\" target=\"_blank\">Part-based Face Detector and Pose Estimation</a>&nbsp;– Implementation of a unified approach for face detection, pose estimation, and landmark localization (CVPR 2012).</p>\n<p>&nbsp;</p>\n<p>Attributes and Semantic Features</p>\n<ul>\n<li>\n<p><a href=\"http://ttic.uchicago.edu/~dparikh/relative.html#code\" target=\"_blank\">Relative Attributes</a>&nbsp;– Modified implementation of RankSVM to train Relative Attributes (ICCV 2011).</p>\n</li>\n<li>\n<p><a href=\"http://vision.stanford.edu/projects/objectbank/\" target=\"_blank\">Object Bank</a>&nbsp;– Implementation of object bank semantic features (NIPS 2010). See also&nbsp;<a href=\"http://www.cse.buffalo.edu/~jcorso/r/actionbank/\" target=\"_blank\">ActionBank</a></p>\n</li>\n<li>\n<p><a href=\"http://vlg.cs.dartmouth.edu/projects/vlg_extractor/vlg_extractor/Home.html\" target=\"_blank\">Classemes, Picodes, and Meta-class features</a>&nbsp;– Software for extracting high-level image descriptors (ECCV 2010, NIPS 2011, CVPR 2012).</p>\n</li>\n</ul>\n<p>Large-Scale Learning</p>\n<ul>\n<li>\n<p><a href=\"http://ttic.uchicago.edu/~smaji/projects/fiksvm/\" target=\"_blank\">Additive Kernels</a>&nbsp;– Source code for fast additive kernel SVM classifiers (PAMI 2013).</p>\n</li>\n<li>\n<p><a href=\"http://www.csie.ntu.edu.tw/~cjlin/liblinear/\" target=\"_blank\">LIBLINEAR</a>&nbsp;– Library for large-scale linear SVM classification.</p>\n</li>\n<li>\n<p><a href=\"http://www.vlfeat.org/\" target=\"_blank\">VLFeat</a>&nbsp;– Implementation for Pegasos SVM and Homogeneous Kernel map.</p>\n</li>\n</ul>\n<p>Fast Indexing and Image Retrieval</p>\n<ul>\n<li>\n<p><a href=\"http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN\" target=\"_blank\">FLANN</a>&nbsp;– Library for performing fast approximate nearest neighbor.</p>\n</li>\n<li>\n<p><a href=\"http://www.cse.ohio-state.edu/~kulis/klsh/klsh.htm\" target=\"_blank\">Kernelized LSH</a>&nbsp;– Source code for Kernelized Locality-Sensitive Hashing (ICCV 2009).</p>\n</li>\n<li>\n<p><a href=\"http://www.unc.edu/~yunchao/itq.htm\" target=\"_blank\">ITQ Binary codes</a>&nbsp;– Code for generation of small binary codes using Iterative Quantization and other baselines such as Locality-Sensitive-Hashing (CVPR 2011).</p>\n</li>\n<li>\n<p><a href=\"http://lear.inrialpes.fr/src/inria_fisher/\" target=\"_blank\">INRIA Image Retrieval</a>&nbsp;– Efficient code for state-of-the-art large-scale image retrieval (CVPR 2011).</p>\n</li>\n</ul>\n<p>Object Detection</p>\n<ul>\n<li>\n<p>See&nbsp;<a href=\"http://rogerioferis.com/VisualRecognitionAndSearch/Resources.html#parts\" target=\"_blank\">Part-based Models</a>&nbsp;and&nbsp;<a href=\"http://rogerioferis.com/VisualRecognitionAndSearch/Resources.html#convnets\" target=\"_blank\">Convolutional Nets</a>&nbsp;above.</p>\n</li>\n<li>\n<p><a href=\"https://bitbucket.org/rodrigob/doppia\" target=\"_blank\">Pedestrian Detection at 100fps</a>&nbsp;– Very fast and accurate pedestrian detector (CVPR 2012).</p>\n</li>\n<li>\n<p><a href=\"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\" target=\"_blank\">Caltech Pedestrian Detection Benchmark</a>&nbsp;– Excellent resource for pedestrian detection, with various links for state-of-the-art implementations.</p>\n</li>\n<li>\n<p><a href=\"http://docs.opencv.org/trunk/modules/objdetect/doc/cascade_classification.html?highlight=face%20detection\" target=\"_blank\">OpenCV</a>&nbsp;– Enhanced implementation of Viola&amp;Jones real-time object detector, with trained models for face detection.</p>\n</li>\n<li>\n<p><a href=\"https://sites.google.com/site/christophlampert/software\" target=\"_blank\">Efficient Subwindow Search</a>&nbsp;– Source code for branch-and-bound optimization for efficient object localization (CVPR 2008).</p>\n</li>\n</ul>\n<p>3D Recognition</p>\n<ul>\n<li>\n<p><a href=\"http://www.pointclouds.org/\" target=\"_blank\">Point-Cloud Library</a>&nbsp;– Library for 3D image and point cloud processing.</p>\n</li>\n</ul>\n<p>Action Recognition</p>\n<ul>\n<li>\n<p><a href=\"http://www.cse.buffalo.edu/~jcorso/r/actionbank/\" target=\"_blank\">ActionBank</a>&nbsp;– Source code for action recognition based on the ActionBank representation (CVPR 2012).</p>\n</li>\n<li>\n<p><a href=\"http://www.di.ens.fr/~laptev/download.html\" target=\"_blank\">STIP Features</a>&nbsp;– software for computing space-time interest point descriptors</p>\n</li>\n<li>\n<p><a href=\"http://ai.stanford.edu/~quocle/\" target=\"_blank\">Independent Subspace Analysis</a>&nbsp;– Look for Stacked ISA for Videos (CVPR 2011)</p>\n</li>\n<li>\n<p><a href=\"http://www.cs.rochester.edu/~rmessing/uradl/\" target=\"_blank\">Velocity Histories of Tracked Keypoints</a>&nbsp;- C++ code for activity recognition using the velocity histories of tracked keypoints (ICCV 2009)</p>\n</li>\n</ul>\n<hr class=\"l\">\n<p>Datasets</p>\n<p>Attributes</p>\n<ul>\n<li>\n<p><a href=\"http://attributes.kyb.tuebingen.mpg.de/\" target=\"_blank\">Animals with Attributes</a>&nbsp;– 30,475 images of 50 animals classes with 6 pre-extracted feature representations for each image.</p>\n</li>\n<li>\n<p><a href=\"http://vision.cs.uiuc.edu/attributes/\" target=\"_blank\">aYahoo and aPascal</a>&nbsp;– Attribute annotations for images collected from Yahoo and Pascal VOC 2008.</p>\n</li>\n<li>\n<p><a href=\"http://www.cs.columbia.edu/CAVE/databases/facetracer/\" target=\"_blank\">FaceTracer</a>&nbsp;– 15,000 faces annotated with 10 attributes and fiducial points.</p>\n</li>\n<li>\n<p><a href=\"http://www.cs.columbia.edu/CAVE/databases/pubfig/\" target=\"_blank\">PubFig</a>&nbsp;– 58,797 face images of 200 people with 73 attribute classifier outputs.</p>\n</li>\n<li>\n<p>[url=http://vis-<a href=\"http://www.cs.umass.edu/lfw/\" target=\"_blank\">www.cs.umass.edu/lfw/</a>]LFW[/url]&nbsp;– 13,233 face images of 5,749 people with 73 attribute classifier outputs.</p>\n</li>\n<li>\n<p><a href=\"http://www.eecs.berkeley.edu/~lbourdev/poselets/\" target=\"_blank\">Human Attributes</a>&nbsp;– 8,000 people with annotated attributes. Check also this&nbsp;<a href=\"https://sharma.users.greyc.fr/hatdb/\" target=\"_blank\">link</a>&nbsp;for another dataset of human attributes.</p>\n</li>\n<li>\n<p><a href=\"http://cs.brown.edu/~gen/sunattributes.html\" target=\"_blank\">SUN Attribute Database</a>&nbsp;– Large-scale scene attribute database with a taxonomy of 102 attributes.</p>\n</li>\n<li>\n<p><a href=\"http://www.image-net.org/download-attributes\" target=\"_blank\">ImageNet Attributes</a>&nbsp;– Variety of attribute labels for the ImageNet dataset.</p>\n</li>\n<li>\n<p><a href=\"http://ttic.uchicago.edu/~dparikh/relative.html#data\" target=\"_blank\">Relative attributes</a>&nbsp;– Data for OSR and a subset of PubFig datasets. Check also this&nbsp;<a href=\"http://vision.cs.utexas.edu/whittlesearch/\" target=\"_blank\">link</a>&nbsp;for the WhittleSearch data.</p>\n</li>\n<li>\n<p><a href=\"http://tamaraberg.com/attributesDataset/index.html\" target=\"_blank\">Attribute Discovery Dataset</a>&nbsp;– Images of shopping categories associated with textual descriptions.</p>\n</li>\n</ul>\n<p>Fine-grained Visual Categorization</p>\n<ul>\n<li>\n<p><a href=\"http://www.vision.caltech.edu/visipedia/CUB-200-2011.html\" target=\"_blank\">Caltech-UCSD Birds Dataset</a>&nbsp;– Hundreds of bird categories with annotated parts and attributes.</p>\n</li>\n<li>\n<p><a href=\"http://vision.stanford.edu/aditya86/ImageNetDogs/\" target=\"_blank\">Stanford Dogs Dataset</a>&nbsp;– 20,000 images of 120 breeds of dogs from around the world.</p>\n</li>\n<li>\n<p><a href=\"http://www.robots.ox.ac.uk/~vgg/data/pets/\" target=\"_blank\">Oxford-IIIT Pet Dataset</a>&nbsp;– 37 category pet dataset with roughly 200 images for each class. Pixel level trimap segmentation is included.</p>\n</li>\n<li>\n<p><a href=\"http://www.comp.leeds.ac.uk/scs6jwks/dataset/leedsbutterfly/\" target=\"_blank\">Leeds Butterfly Dataset</a>&nbsp;– 832 images of 10 species of butterflies.</p>\n</li>\n<li>\n<p><a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/\" target=\"_blank\">Oxford Flower Dataset</a>&nbsp;– Hundreds of flower categories.</p>\n</li>\n</ul>\n<p>Face Detection</p>\n<ul>\n<li>\n<p>[url=http://vis-<a href=\"http://www.cs.umass.edu/fddb/\" target=\"_blank\">www.cs.umass.edu/fddb/</a>]FDDB[/url]&nbsp;– UMass face detection dataset and benchmark (5,000+ faces)</p>\n</li>\n<li>\n<p><a href=\"http://vasc.ri.cmu.edu/idb/html/face/frontal_images/index.html\" target=\"_blank\">CMU/MIT</a>&nbsp;– Classical face detection dataset.</p>\n</li>\n</ul>\n<p>Face Recognition</p>\n<ul>\n<li>\n<p><a href=\"http://www.face-rec.org/databases/\" target=\"_blank\">Face Recognition Homepage</a>&nbsp;– Large collection of face recognition datasets.</p>\n</li>\n<li>\n<p>[url=http://vis-<a href=\"http://www.cs.umass.edu/lfw/\" target=\"_blank\">www.cs.umass.edu/lfw/</a>]LFW[/url]&nbsp;– UMass unconstrained face recognition dataset (13,000+ face images).</p>\n</li>\n<li>\n<p><a href=\"http://www.nist.gov/itl/iad/ig/face.cfm\" target=\"_blank\">NIST Face Homepage</a>&nbsp;– includes face recognition grand challenge (FRGC), vendor tests (FRVT) and others.</p>\n</li>\n<li>\n<p><a href=\"http://www.multipie.org/\" target=\"_blank\">CMU Multi-PIE</a>&nbsp;– contains more than 750,000 images of 337 people, with 15 different views and 19 lighting conditions.</p>\n</li>\n<li>\n<p><a href=\"http://www.nist.gov/itl/iad/ig/colorferet.cfm\" target=\"_blank\">FERET</a>&nbsp;– Classical face recognition dataset.</p>\n</li>\n<li>\n<p><a href=\"http://www.cad.zju.edu.cn/home/dengcai/Data/FaceData.html\" target=\"_blank\">Deng Cai’s face dataset in Matlab Format</a>&nbsp;– Easy to use if you want play with simple face datasets including Yale, ORL, PIE, and Extended Yale B.</p>\n</li>\n<li>\n<p><a href=\"http://www.scface.org/\" target=\"_blank\">SCFace</a>&nbsp;– Low-resolution face dataset captured from surveillance cameras.</p>\n</li>\n</ul>\n<p>Handwritten Digits</p>\n<ul>\n<li>\n<p><a href=\"http://yann.lecun.com/exdb/mnist/\" target=\"_blank\">MNIST</a>&nbsp;– large dataset containing a training set of 60,000 examples, and a test set of 10,000 examples.</p>\n</li>\n</ul>\n<p>Pedestrian Detection</p>\n<ul>\n<li>\n<p><a href=\"http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/\" target=\"_blank\">Caltech Pedestrian Detection Benchmark</a>&nbsp;– 10 hours of video taken from a vehicle,350K bounding boxes for about 2.3K unique pedestrians.</p>\n</li>\n<li>\n<p><a href=\"http://pascal.inrialpes.fr/data/human/\" target=\"_blank\">INRIA Person Dataset</a>&nbsp;– Currently one of the most popular pedestrian detection datasets.</p>\n</li>\n<li>\n<p><a href=\"http://www.vision.ee.ethz.ch/~aess/dataset/\" target=\"_blank\">ETH Pedestrian Dataset</a>&nbsp;– Urban dataset captured from a stereo rig mounted on a stroller.</p>\n</li>\n<li>\n<p><a href=\"http://www.d2.mpi-inf.mpg.de/tud-brussels\" target=\"_blank\">TUD-Brussels Pedestrian Dataset</a>&nbsp;– Dataset with image pairs recorded in an crowded urban setting with an onboard camera.</p>\n</li>\n<li>\n<p><a href=\"http://pascallin.ecs.soton.ac.uk/challenges/VOC/\" target=\"_blank\">PASCAL Human Detection</a>&nbsp;– One of 20 categories in PASCAL VOC detection challenges.</p>\n</li>\n<li>\n<p><a href=\"http://iris.usc.edu/Vision-Users/OldUsers/bowu/DatasetWebpage/dataset.html\" target=\"_blank\">USC Pedestrian Dataset</a>&nbsp;– Small dataset captured from surveillance cameras.</p>\n</li>\n</ul>\n<p>Generic Object Recognition</p>\n<ul>\n<li>\n<p><a href=\"http://www.image-net.org/\" target=\"_blank\">ImageNet</a>&nbsp;– Currently the largest visual recognition dataset in terms of number of categories and images.</p>\n</li>\n<li>\n<p><a href=\"http://groups.csail.mit.edu/vision/TinyImages/\" target=\"_blank\">Tiny Images</a>&nbsp;– 80 million 32x32 low resolution images.</p>\n</li>\n<li>\n<p><a href=\"http://pascallin.ecs.soton.ac.uk/challenges/VOC/\" target=\"_blank\">Pascal VOC</a>&nbsp;– One of the most influential visual recognition datasets.</p>\n</li>\n<li>\n<p><a href=\"http://www.vision.caltech.edu/Image_Datasets/Caltech101/\" target=\"_blank\">Caltech 101</a>&nbsp;/&nbsp;<a href=\"http://www.vision.caltech.edu/Image_Datasets/Caltech256/\" target=\"_blank\">Caltech 256</a>&nbsp;– Popular image datasets containing 101 and 256 object categories, respectively.</p>\n</li>\n<li>\n<p><a href=\"http://new-labelme.csail.mit.edu/Release3.0/index.php\" target=\"_blank\">MIT LabelMe</a>&nbsp;– Online annotation tool for building computer vision databases.</p>\n</li>\n</ul>\n<p>Scene Recognition</p>\n<ul>\n<li>\n<p><a href=\"http://groups.csail.mit.edu/vision/SUN/\" target=\"_blank\">MIT SUN Dataset</a>&nbsp;– MIT scene understanding dataset.</p>\n</li>\n<li>\n<p><a href=\"http://www-cvr.ai.uiuc.edu/ponce_grp/data/\" target=\"_blank\">UIUC Fifteen Scene Categories</a>&nbsp;– Dataset of 15 natural scene categories.</p>\n</li>\n</ul>\n<p>Feature Detection and Description</p>\n<ul>\n<li>\n<p><a href=\"http://www.robots.ox.ac.uk/~vgg/data/data-aff.html\" target=\"_blank\">VGG Affine Dataset</a>&nbsp;– Widely used dataset for measuring performance of feature detection and description. Check<a href=\"http://www.vlfeat.org/benchmarks/index.html\" target=\"_blank\">VLBenchmarks</a>for an evaluation framework.</p>\n</li>\n</ul>\n<p>Action Recognition</p>\n<ul>\n<li>\n<p><a href=\"http://rogerioferis.com/VisualRecognitionAndSearch/material/LiuFerisSunTutorial.pdf\" target=\"_blank\">Benchmarking Activity Recognition</a>&nbsp;– CVPR 2012 tutorial covering various datasets for action recognition.</p>\n</li>\n</ul>\n<p>RGBD Recognition</p>\n<ul>\n<li>\n<p><a href=\"http://www.cs.washington.edu/rgbd-dataset/index.html\" target=\"_blank\">RGB-D Object Dataset</a>&nbsp;– Dataset containing 300 common household objects</p>\n</li>\n</ul>\n<p>Reference:</p>\n<p>&nbsp;</p>\n<p>[1]:&nbsp;<a href=\"http://rogerioferis.com/VisualRecognitionAndSearch/Resources.html\" target=\"_blank\">http://rogerioferis.com/VisualRecognitionAndSearch/Resources.html</a></p>\n<p><br>特征提取</p>\n<ul>\n<li>\n<p>SURF特征:&nbsp;<a href=\"http://www.vision.ee.ethz.ch/software/index.de.html(%E5%BD%93%E7%84%B6%E8%BF%99%E5%8F%AA%E6%98%AF%E5%85%B6%E4%B8%AD%E4%B9%8B%E4%B8%80\" target=\"_blank\">http://www.vision.ee.ethz.ch/software/index.de.html</a>(当然这只是其中之一)</p>\n</li>\n<li>\n<p>LBP特征(一种纹理特征)：<a href=\"http://www.comp.hkbu.edu.hk/~icpr06/tutorials/Pietikainen.html\" target=\"_blank\">http://www.comp.hkbu.edu.hk/~icpr06/tutorials/Pietikainen.html</a></p>\n</li>\n<li>\n<p>Fast Corner Detection（OpenCV中的Fast算法）:<a href=\"http://mi.eng.cam.ac.uk/~er258/work/fast.html\" target=\"_blank\">FAST Corner Detection -- Edward Rosten</a></p>\n</li>\n</ul>\n<p>机器视觉</p>\n<ul>\n<li>\n<p>A simple object detector with boosting(Awarded the Best Short Course Prize at ICCV 2005，So了解adaboost的推荐之作)：<a href=\"http://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html\" target=\"_blank\">http://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html</a></p>\n</li>\n<li>\n<p>Boosting(该网页上有相当全的Boosting的文章和几个Boosting代码，本人推荐)：<a href=\"http://cbio.mskcc.org/~aarvey/boosting_papers.html\" target=\"_blank\">http://cbio.mskcc.org/~aarvey/boosting_papers.html</a></p>\n</li>\n<li>\n<p>Adaboost Matlab 工具：<a href=\"http://graphics.cs.msu.ru/en/science/research/machinelearning/adaboosttoolbox\" target=\"_blank\">http://graphics.cs.msu.ru/en/science/research/machinelearning/adaboosttoolbox</a></p>\n</li>\n<li>\n<p><a href=\"http://192.168.1.27/wiki/MultiBoost\" target=\"_blank\">MultiBoost</a>(不说啥了，多类Adaboost算法的程序)：<a href=\"http://sourceforge.net/projects/multiboost/\" target=\"_blank\">http://sourceforge.net/projects/multiboost/</a></p>\n</li>\n<li>\n<p><a href=\"http://192.168.1.27/wiki/TextonBoost\" target=\"_blank\">TextonBoost</a>(我们教研室王冠夫师兄的毕设):&nbsp;<a href=\"http://jamie.shotton.org/work/code.html\" target=\"_blank\">Jamie Shotton - Code</a></p>\n</li>\n<li>\n<p><a href=\"http://192.168.1.27/wiki/LibSvm\" target=\"_blank\">LibSvm</a>的老爹（推荐）:&nbsp;<a href=\"http://www.csie.ntu.edu.tw/~cjlin/\" target=\"_blank\">http://www.csie.ntu.edu.tw/~cjlin/</a></p>\n</li>\n<li>\n<p><a href=\"http://www.inference.phy.cam.ac.uk/hmw26/crf/\" target=\"_blank\">Conditional Random Fields</a>（CRF论文+Code列表，推荐）</p>\n</li>\n<li>\n<p><a href=\"http://crfpp.sourceforge.net/\" target=\"_blank\">CRF++: Yet Another CRF toolkit</a></p>\n</li>\n<li>\n<p><a href=\"http://www.computervisiononline.com/software/conditional-random-field-crf-toolbox-matlab\" target=\"_blank\">Conditional Random Field (CRF) Toolbox for Matlab</a></p>\n</li>\n<li>\n<p><a href=\"http://www.cs.cmu.edu/~jkbradle/TreeCRFs/\" target=\"_blank\">Tree CRFs</a></p>\n</li>\n<li>\n<p><a href=\"http://alias-i.com/lingpipe/web/install.html\" target=\"_blank\">LingPipe: Installation</a></p>\n</li>\n<li>\n<p><a href=\"http://jedlik.phy.bme.hu/~gerjanos/HMM/node2.html\" target=\"_blank\">Hidden Markov Models</a>（推荐）</p>\n</li>\n<li>\n<p><a href=\"http://blog.csdn.net/eaglex/article/details/6376826\" target=\"_blank\">隐马尔科夫模型</a><a href=\"http://blog.csdn.net/eaglex/article/details/6376826\" target=\"_blank\">(Hidden Markov Models)</a><a href=\"http://blog.csdn.net/eaglex/article/details/6376826\" target=\"_blank\">系列之一</a><a href=\"http://blog.csdn.net/eaglex/article/details/6376826\" target=\"_blank\">&nbsp;- eaglex</a><a href=\"http://blog.csdn.net/eaglex/article/details/6376826\" target=\"_blank\">的专栏 - 博客频道&nbsp;</a><a href=\"http://blog.csdn.net/eaglex/article/details/6376826\" target=\"_blank\">- CSDN.NET</a><a href=\"http://blog.csdn.net/eaglex/article/details/6376826\" target=\"_blank\">（推荐）</a></p>\n</li>\n</ul>\n<p>综合代码</p>\n<ul>\n<li>\n<p><a href=\"http://192.168.1.27/wiki/CvPapers\" target=\"_blank\">CvPapers</a>(好吧，牛吧网站，里面有ICCV，CVPR，ECCV，SIGGRAPH的论文收录，然后还有一些论文的代码搜集，要求加精！)：<a href=\"http://www.cvpapers.com/\" target=\"_blank\">http://www.cvpapers.com/</a></p>\n</li>\n<li>\n<p>Computer Vision Software(里面代码很多，并详细的给出了分类)：<a href=\"http://peipa.essex.ac.uk/info/software.html\" target=\"_blank\">http://peipa.essex.ac.uk/info/software.html</a></p>\n</li>\n<li>\n<p>某人的Windows Live（我看里面东东不少就收藏了）：<a href=\"https://skydrive.live.com/?cid=3b6244088fd5a769#cid=3B6244088FD5A769&amp;id=3B6244088FD5A769!523\" target=\"_blank\">https://skydrive.live.com/?cid=3b6244088fd5a769#cid=3B6244088FD5A769&amp;id=3B6244088FD5A769!523</a></p>\n</li>\n<li>\n<p>MATLAB and Octave Functions for Computer Vision and Image Processing（这个里面的东西也很全，只是都是用Matlab和Octave开发的）：<a href=\"http://www.csse.uwa.edu.au/~pk/research/matlabfns/\" target=\"_blank\">http://www.csse.uwa.edu.au/~pk/research/matlabfns/</a></p>\n</li>\n<li>\n<p>Computer Vision Resources（里面的视觉算法很多，给出了相应的论文和Code，挺好的）：<a href=\"https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html\" target=\"_blank\">https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html</a></p>\n</li>\n<li>\n<p>MATLAB Functions for Multiple View Geometry（关于物体多视角计算的库）：<a href=\"http://www.robots.ox.ac.uk/~vgg/hzbook/code/\" target=\"_blank\">http://www.robots.ox.ac.uk/~vgg/hzbook/code/</a></p>\n</li>\n<li>\n<p>Evolutive Algorithm based on Naïve Bayes models Estimation（单独列了一个算法的Code）：<a href=\"http://www.cvc.uab.cat/~xbaro/eanbe/#_Software\" target=\"_blank\">http://www.cvc.uab.cat/~xbaro/eanbe/#_Software</a></p>\n</li>\n</ul>\n<p>主页代码</p>\n<ul>\n<li>\n<p><a href=\"http://pablonegri.free.fr/index.html\" target=\"_blank\">Pablo Negri's Home Page</a></p>\n</li>\n<li>\n<p><a href=\"http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\" target=\"_blank\">Jianxin Wu's homepage</a></p>\n</li>\n<li>\n<p><a href=\"http://www.cs.ubc.ca/~pcarbo/\" target=\"_blank\">Peter Carbonetto</a></p>\n</li>\n<li>\n<p><a href=\"http://people.csail.mit.edu/billf/project%20pages/sresCode/Markov%20Random%20Fields%20for%20Super-Resolution.html\" target=\"_blank\">Markov Random Fields for Super-Resolution</a></p>\n</li>\n<li>\n<p><a href=\"http://www.wisdom.weizmann.ac.il/~vision/SketchTheCommon/\" target=\"_blank\">Detecting and Sketching the Common</a></p>\n</li>\n<li>\n<p><a href=\"http://people.cs.uchicago.edu/~pff/\" target=\"_blank\">Pedro Felzenszwalb</a></p>\n</li>\n<li>\n<p><a href=\"http://users.soe.ucsc.edu/~rokaf/interests.html\" target=\"_blank\">Hae JONG, SEO</a></p>\n</li>\n<li>\n<p><a href=\"http://www.cise.ufl.edu/class/cap5416fa09/Projects.html\" target=\"_blank\">CAP 5416 - Computer Vision</a></p>\n</li>\n<li>\n<p><a href=\"http://www.robots.ox.ac.uk/~gk/PTAM/\" target=\"_blank\">Parallel Tracking and Mapping for Small AR Workspaces (PTAM)</a></p>\n</li>\n<li>\n<p><a href=\"http://www.ics.uci.edu/~dramanan/\" target=\"_blank\">Deva Ramanan - UC Irvine - Computer Vision</a></p>\n</li>\n<li>\n<p><a href=\"http://www.umiacs.umd.edu/~raghuram/\" target=\"_blank\">Raghuraman Gopalan</a></p>\n</li>\n<li>\n<p><a href=\"http://bmi.osu.edu/~hkong/index.htm\" target=\"_blank\">Hui Kong</a></p>\n</li>\n<li>\n<p><a href=\"http://jamie.shotton.org/work/index.html\" target=\"_blank\">Jamie Shotton - Post-Doctoral Researcher in Computer Vision</a></p>\n</li>\n<li>\n<p><a href=\"http://imagine.enpc.fr/~audibert/index.html\" target=\"_blank\">Jean-Yves AUDIBERT</a></p>\n</li>\n<li>\n<p><a href=\"http://www.csd.uwo.ca/~olga/\" target=\"_blank\">Olga Veksler</a></p>\n</li>\n<li>\n<p><a href=\"http://users.cecs.anu.edu.au/~sgould/index.html#software\" target=\"_blank\">Stephen Gould</a></p>\n</li>\n<li>\n<p><a href=\"http://faculty.ucmerced.edu/mhyang/code.html\" target=\"_blank\">Publications (Last Update: 09/30/10)</a></p>\n</li>\n<li>\n<p><a href=\"http://cvlab.epfl.ch/~ali/flowboost.htm\" target=\"_blank\">Karim Ali - FlowBoost</a></p>\n</li>\n<li>\n<p><a href=\"http://people.csail.mit.edu/fergus/iccv2005/partsstructure.html\" target=\"_blank\">A simple parts and structure object detector</a></p>\n</li>\n<li>\n<p><a href=\"http://cms.brookes.ac.uk/research/visiongroup/code.php\" target=\"_blank\">Code - Oxford Brookes Vision Group</a></p>\n</li>\n<li>\n<p><a href=\"http://chasen.org/~taku/index.html.en\" target=\"_blank\">Taku Kudo</a></p>\n</li>\n</ul>\n<p>行人检测</p>\n<ul>\n<li>\n<p><a href=\"http://www.computing.edu.au/~12482661/hog.html\" target=\"_blank\">Histogram of Oriented Gradient (Windows)</a></p>\n</li>\n<li>\n<p><a href=\"http://www.cs.berkeley.edu/~smaji/projects/ped-detector/\" target=\"_blank\">INRIA Pedestrian detector</a></p>\n</li>\n<li>\n<p><a href=\"http://www.eecs.berkeley.edu/~lbourdev/poselets/\" target=\"_blank\">Poselets</a></p>\n</li>\n<li>\n<p><a href=\"http://www.liv.ic.unicamp.br/~wschwartz/softwares.html\" target=\"_blank\">William Robson Schwartz - Softwares</a></p>\n</li>\n<li>\n<p><a href=\"http://www.vision.ee.ethz.ch/~calvin/calvin_upperbody_detector/\" target=\"_blank\">calvin upper-body detector v1.02</a></p>\n</li>\n<li>\n<p><a href=\"http://www.cvg.rdg.ac.uk/software/rpt/index.html\" target=\"_blank\">RPT@CVG</a></p>\n</li>\n<li>\n<p><a href=\"http://www.idiap.ch/~odobez/human-detection/index.html\" target=\"_blank\">Main Page</a></p>\n</li>\n<li>\n<p><a href=\"http://www.lienhart.de/Source_Code/source_code.html\" target=\"_blank\">Source Code</a></p>\n</li>\n<li>\n<p><a href=\"http://www.informatik.uni-freiburg.de/~spinello/people2D.html\" target=\"_blank\">Dr. Luciano Spinello</a></p>\n</li>\n<li>\n<p><a href=\"http://bmi.osu.edu/~hkong/Human_Detection.html\" target=\"_blank\">Pedestrian Detection</a></p>\n</li>\n<li>\n<p><a href=\"http://www.vision.ee.ethz.ch/~gallju/projects/houghforest/index.html\" target=\"_blank\">Class-Specific Hough Forests for Object Detection</a></p>\n</li>\n<li>\n<p><a href=\"http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\" target=\"_blank\">Jianxin Wu's homepage</a>（就是上面的）</p>\n</li>\n<li>\n<p>Berkeley大学做的Pedestrian Detector，使用交叉核的支持向量机，特征使用HOG金字塔，提供Matlab和C++混编的代码：<a href=\"http://www.cs.berkeley.edu/~smaji/projects/ped-detector/\" target=\"_blank\">http://www.cs.berkeley.edu/~smaji/projects/ped-detector/</a></p>\n</li>\n</ul>\n<p>视觉壁障</p>\n<ul>\n<li>\n<p><a href=\"http://www.cs.cornell.edu/~asaxena/rccar/\" target=\"_blank\">High Speed Obstacle Avoidance using Monocular Vision and Reinforcement Learning</a></p>\n</li>\n<li>\n<p><a href=\"http://info.ee.surrey.ac.uk/Personal/Z.Kalal/tld.html\" target=\"_blank\">TLD</a>(2010年很火的tracking算法)</p>\n</li>\n<li>\n<p><a href=\"http://www.vision.ee.ethz.ch/boostingTrackers/\" target=\"_blank\">online boosting trackers</a></p>\n</li>\n<li>\n<p><a href=\"http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml\" target=\"_blank\">Boris Babenko</a></p>\n</li>\n<li>\n<p>Optical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递 归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)<a href=\"http://of-.sourceforge.net/\" target=\"_blank\">http://of-eval.sourceforge.net/</a></p>\n</li>\n</ul>\n<p>物体检测算法</p>\n<ul>\n<li>\n<p><a href=\"http://www.irisa.fr/vista/Equipe/People/Laptev/objectdetection.html\" target=\"_blank\">Object Detection</a></p>\n</li>\n<li>\n<p><a href=\"http://www.seas.upenn.edu/~limingw/obj_det_accv07/code.html\" target=\"_blank\">Software for object detection</a></p>\n</li>\n</ul>\n<p>人脸检测</p>\n<ul>\n<li>\n<p><a href=\"http://www.lienhart.de/Source_Code/source_code.html\" target=\"_blank\">Source Code</a></p>\n</li>\n<li>\n<p><a href=\"http://itp.nyu.edu/~mbe230/blogmer/2011/02/10-face-detection-projects/\" target=\"_blank\">10个人脸检测项目</a></p>\n</li>\n<li>\n<p><a href=\"http://c2inet.sce.ntu.edu.sg/Jianxin/index.html\" target=\"_blank\">Jianxin Wu's homepage</a>（又是这货）</p>\n</li>\n</ul>\n<p>ICA独立成分分析</p>\n<ul>\n<li>\n<p><a href=\"http://cnl.salk.edu/~tony/ica.html\" target=\"_blank\">An ICA page-papers,code,demo,links (Tony Bell)</a></p>\n</li>\n<li>\n<p><a href=\"http://research.ics.tkk.fi/ica/fastica/\" target=\"_blank\">FastICA</a></p>\n</li>\n<li>\n<p><a href=\"http://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html\" target=\"_blank\">Cached k-d tree search for ICP algorithms</a></p>\n</li>\n</ul>\n<p>滤波算法</p>\n<ul>\n<li>\n<p>卡尔曼滤波：<a href=\"http://www.cs.unc.edu/~welch/kalman/index.html\" target=\"_blank\">The Kalman Filter</a>(终极网页)</p>\n</li>\n<li>\n<p>Bayesian Filtering Library:&nbsp;<a href=\"http://www.orocos.org/bfl\" target=\"_blank\">The Bayesian Filtering Library</a></p>\n</li>\n</ul>\n<p>路面识别</p>\n<ul>\n<li>\n<p><a href=\"http://www.multimedia-computing.de/wiki/Source_Code#Dataset_of_logos_in_real-world_images:_FlickrLogos-32\" target=\"_blank\">Source Code</a></p>\n</li>\n<li>\n<p><a href=\"http://bmi.osu.edu/~hkong/Road_Detection.html\" target=\"_blank\">Vanishing point detection for general road detection</a></p>\n</li>\n</ul>\n<p>分割算法</p>\n<ul>\n<li>\n<p>MATLAB Normalized Cuts Segmentation Code：<a href=\"http://www.cis.upenn.edu/~jshi/software/\" target=\"_blank\">software</a></p>\n</li>\n<li>\n<p>超像素分割：<a href=\"http://ivrg.epfl.ch/supplementary_material/RK_SLICSuperpixels/index.html\" target=\"_blank\">SLIC Superpixels</a></p>\n</li>\n</ul>\n","_path":"20210506/ji-suan-ji-shi-jue-niu-ren-bo-ke-he-dai-ma-hui-zong/","_link":"https://yaoqs.github.io/20210506/ji-suan-ji-shi-jue-niu-ren-bo-ke-he-dai-ma-hui-zong/","_id":"clzpq9hyw00aesgerg1kgh6q8"}}