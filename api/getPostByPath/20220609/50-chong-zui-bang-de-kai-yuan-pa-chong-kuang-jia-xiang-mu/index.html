{"type":"getPostByPath","data":{"title":"50种最棒的开源爬虫框架/项目","date":"2022-06-09T15:40:26.000Z","description":"","categories":[{"name":"爬虫","_id":"clzpq9hs6000esgerhky5dowk"}],"tags":[{"name":"爬虫","_id":"clzpq9hs7000fsgere3hx8skh"}],"content":"<p>作者：<strong><a href=\"https://link.zhihu.com/?target=http%3A//www.prowebscraper.com/blog/50-best-open-source-web-crawlers/\">Prowebscraper 博客</a></strong></p>\n<p>译者：Rays</p>\n<p><strong>摘要：</strong> 说起爬虫框架，你可能会马上脱口而出：「 Scrapy 或者 Pyspider」，甚至你可能认为只有 Python 才能爬虫。其实还有很多好用的开源爬虫框架，也绝不仅仅只有 Python 才能写爬虫，大多数热门语言都可以做。</p>\n<p>总之，开源 Web 爬虫纷繁多样，下面按照所用程语言，罗列五十种最好的开源爬虫框架，每一个各具特长，适用于不同场景和用户需求。下面来一睹为快。</p>\n<p><img src=\"https://pic1.zhimg.com/v2-f55e5381062a9d4ae47e769e7f8313d8_r.jpg\" alt=\"\"></p>\n<h2 id=\"Python-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#Python-编写的开源-Web-爬虫\">※</a><strong>Python 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"1-Scrapy\"><a class=\"header-anchor\" href=\"#1-Scrapy\">※</a><strong>1. Scrapy</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：Python</li>\n<li><strong>GitHub Star 数</strong>：28660</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/v2-0b652a141850fd3b89a3fe101f329e95_r.jpg\" alt=\"\"></p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Scrapy 是一种高速的高层 Web 爬取和 Web 采集框架，可用于爬取网站页面，并从页面中抽取结构化数据。</li>\n<li>Scrapy 的用途广泛，适用于从数据挖掘、监控到自动化测试。</li>\n<li>Scrapy 设计上考虑了从网站抽取特定的信息，它支持使用 CSS 选择器和 XPath 表达式，使开发人员可以聚焦于实现数据抽取。</li>\n<li>对于熟悉 Python 的开发人员，只需几分钟就能建立并运行 Scrapy。</li>\n<li>支持运行在 Linux、Mac OS 和 Windows 系统上。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>内置支持从 HTML 和 XML 抽取数据、使用扩展的 CSS 选择器（Selector）和 XPath 表达式等特性。</li>\n<li>支持以多种格式（JSON、CSV、XML）生成输出。</li>\n<li>基于 Twisted 构建。</li>\n<li>稳健的支持，自动检测编码方式。</li>\n<li>快速，功能强大。</li>\n</ul>\n<p>– <strong>官方文档</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//docs.scrapy.org/en/latest/\">https://docs.scrapy.org/en/latest/</a></strong></p>\n<p>– <strong>官方网站</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//scrapy.org/\">https://scrapy.org/</a></strong></p>\n<h3 id=\"2-Cola\"><a class=\"header-anchor\" href=\"#2-Cola\">※</a><strong>2. Cola</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：Python</li>\n<li><strong>GitHub Star 数</strong>：1274</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Cola 是一种高层分布式爬取框架，实现从网站爬取网页，并从中抽取结构化数据。</li>\n<li>它提供了一种实现目标数据获取的简单且灵活的方式。</li>\n<li>用户只需要编写其中一部分代码，就可在本地和分布式环境下运行。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>高层分布式爬取框架。</li>\n<li>简单且高速。</li>\n<li>灵活。</li>\n</ul>\n<p>– <strong>官方文档</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/chineking/cola\">https://github.com/chineking/cola</a></strong></p>\n<p>– <strong>官方网站</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//pypi.org/project/Cola/\">https://pypi.org/project/Cola/</a></strong></p>\n<h3 id=\"3-Crawley\"><a class=\"header-anchor\" href=\"#3-Crawley\">※</a><strong>3. Crawley</strong></h3>\n<ul>\n<li><strong>实现语言</strong> Python</li>\n<li><strong>GitHub Star 数</strong>： 144</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Crawley 是一种 Python 爬取和采集框架，意在简化开发人员从 Web 网页抽取数据到数据库等结构化存储中。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>基于 Eventlet 构建的高速 Web 爬虫。</li>\n<li>支持 MySQL、PostgreSQL、Oracle、Sqlite 等关系数据库引擎。</li>\n<li>支持 MongoDB、CouchDB 等 NoSQL 数据库（最新特性！）。</li>\n<li>支持导出数据为 JSON、XML 和 CSV 格式（最新特性！）。</li>\n<li>命令行工具。</li>\n<li>支持开发人员使用自己喜好的工具，例如 XPath 或 Pyquery（一种类似于 JQuery 的 Python 软件库）等。</li>\n<li>支持 Cookie 处理器（Handler）。</li>\n</ul>\n<p>– <strong>官方文档</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//pythonhosted.org/crawley/\">https://pythonhosted.org/crawley/</a></strong></p>\n<p>– <strong>官方网站</strong>：<strong><a href=\"https://link.zhihu.com/?target=http%3A//project.crawley-cloud.com/\">http://project.crawley-cloud.com/</a></strong></p>\n<h3 id=\"4-MechanicalSoup\"><a class=\"header-anchor\" href=\"#4-MechanicalSoup\">※</a><strong>4. MechanicalSoup</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Python</li>\n<li><strong>GitHub Star 数</strong>： 2803</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>MechanicalSoup 是一种设计模拟人类使用 Web 浏览器行为的 Python 软件库，它基于解析软件库 BeautifulSoup 构建。</li>\n<li>如果开发人员需要从单个站点采集数据，或是不需要大量数据采集，那么使用 MechanicalSoup 是一种简单高效的方法。</li>\n<li>MechanicalSoup 自动存储和发送 Cookie、跟踪重定向、支持链接跟随和提交表单。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>轻量级。</li>\n<li>支持 Cookie 处理器。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//mechanicalsoup.readthedocs.io/en/stable/\">https://mechanicalsoup.readthedocs.io/en/stable/</a></strong></p>\n<p>– <strong>官方网站</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//mechanicalsoup.readthedocs.io/\">https://mechanicalsoup.readthedocs.io/</a></strong></p>\n<h3 id=\"5-PySpider\"><a class=\"header-anchor\" href=\"#5-PySpider\">※</a><strong>5. PySpider</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Python</li>\n<li><strong>GitHub Star 数</strong>： 11803</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>PySpider 是一种 Python 编写的强大 Web 爬虫。</li>\n<li>它支持 JavaScript 网页，并具有分布式架构。</li>\n<li>PySpider 支持将爬取数据存储在用户选定的后台数据库，包括**<a href=\"https://link.zhihu.com/?target=https%3A//www.mysql.com/\">MySQL</a>**, <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.mongodb.org/\">MongoDB</a></strong>, <strong><a href=\"https://link.zhihu.com/?target=http%3A//redis.io/\">Redis</a></strong>, <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.sqlite.org/\">SQLite</a></strong>, **<a href=\"https://link.zhihu.com/?target=https%3A//www.elastic.co/\">Elasticsearch</a>**等。</li>\n<li>支持开发人员使用 RabbitMQ、Beanstalk 和 Redis 等作为消息队列。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>提供强大 Web 界面，具有脚本编辑器、任务监控、项目管理器和结果查看器。</li>\n<li>支持对重度 Ajax 网站的爬取。</li>\n<li>易于实现适用、快速的爬取。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//docs.pyspider.org/\">http://docs.pyspider.org/</a></strong></p>\n<p>– <strong>官方网站</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/binux/pyspider\">https://github.com/binux/pyspider</a></strong></p>\n<h3 id=\"6-Portia\"><a class=\"header-anchor\" href=\"#6-Portia\">※</a><strong>6. Portia</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Python</li>\n<li><strong>GitHub Star 数</strong>： 6250</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p>![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='243' height='243'></svg>)</p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Portia 是由 Scrapinghub 创建的一种可视化爬取工具，它不需要用户具有任何程序开发知识。</li>\n<li>如果用户并非开发人员，最好直接使用 Portia 实现 Web 爬取需求。</li>\n<li>用户无需任何安装就可免费试用 Portia，只需要在 Scrapinghub 注册一个账户，就可使用托管版本。</li>\n<li>即便用户没有编程技能，在 Portia 中创建爬虫并抽取 Web 内容也是非常易于实现的。</li>\n<li>用户无需安装任何程序，因为 Portia 是运行在 Web 页面上的。</li>\n<li>用户可以使用 Portia 的基本点击工具标注需要爬取的数据，然后 Portia 就会根据这些标注理解如何爬取类似页面中的数据。</li>\n<li>一旦检测到需要爬取的页面，Portia 会形成一个用户已创建结构的实例。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>通过记录并回放用户在页面上的操作，实现点击、拖动和等待等动作。</li>\n<li>Portia 可以很好地爬取基于 Ajax 构建的网站（基于 Splash），也适用于爬取 Backbone、Angular 和 Ember 等重度 JavsScript 框架。</li>\n</ul>\n<p>– <strong>官方文档</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//portia.readthedocs.io/en/latest/index.html\">https://portia.readthedocs.io/en/latest/index.html</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/scrapinghub/portia\">https://github.com/scrapinghub/portia</a></strong></p>\n<h3 id=\"7-Beautifulsoup\"><a class=\"header-anchor\" href=\"#7-Beautifulsoup\">※</a><strong>7. Beautifulsoup</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Python</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p>![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='250' height='298'></svg>)</p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Beautiful Soup 一种设计用于实现 Web 爬取等快速数据获取项目的 Python 软件库。</li>\n<li>它在设计上处于 HTML 或 XML 解析器之上，提供用于迭代、搜索和修改解析树等功能的 Python 操作原语。往往能为开发人员节省数小时乃至数天的工作。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>Beautiful Soup 自动将输入文档转换为 Unicode 编码，并将输出文档转换为 UTF-8 编码。</li>\n<li>Beautiful Soup 处于一些广为采用的 Python 解析器（例如，<strong><a href=\"https://link.zhihu.com/?target=http%3A//lxml.de/\">lxml</a><strong>和</strong><a href=\"https://link.zhihu.com/?target=http%3A//code.google.com/p/html5lib/\">html5lib</a></strong>）之上，支持用户尝试使用多种不同的解析策略，并在速度和灵活性上做出权衡。</li>\n</ul>\n<p>– <strong>官方文档</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//www.crummy.com/software/BeautifulSoup/bs4/doc/\">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.crummy.com/software/BeautifulSoup/\">https://www.crummy.com/software/BeautifulSoup/</a></strong></p>\n<h3 id=\"8-Spidy-爬虫\"><a class=\"header-anchor\" href=\"#8-Spidy-爬虫\">※</a><strong>8. Spidy 爬虫</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Python</li>\n<li><strong>GitHub Star 数</strong>： 152</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/v2-73a97fabe948e67aba729d3190840aa9_r.jpg\" alt=\"\"></p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Spidy 是一种从命令行运行的 Web 爬虫。易于使用。用户只需提供 Web 网页的 URL 链接，Spidy 就可以开始爬取！Spidy 无疑是一种整体爬取 Web 的简单有效的方式。</li>\n<li>Spidy 使用 Python 请求查询 Web 页面，并使用 lxml 抽取页面中的所有链接。非常简单！</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>错误处理。</li>\n<li>跨平台兼容性。</li>\n<li>频繁时间戳日志。</li>\n<li>可移植性。</li>\n<li>用户友好的日志。</li>\n<li>保存 Web 页面。</li>\n<li>支持文件压缩。</li>\n</ul>\n<p>– <strong>官方文档</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/rivermont/spidy\">https://github.com/rivermont/spidy</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//project.crawley-cloud.com/\">http://project.crawley-cloud.com/</a></strong></p>\n<h3 id=\"9-Garb\"><a class=\"header-anchor\" href=\"#9-Garb\">※</a><strong>9. Garb</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Python</li>\n<li><strong>GitHub Star 数</strong>： 1627</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Grab 是一种用于构建爬虫的 Python 框架。</li>\n<li>使用 Grab 可构建出各种复杂度的 Web 爬虫，从只有五行代码的脚本，到可处理百万量级 Web 页面的复杂异步爬虫。</li>\n<li>Grab 提供了执行网络请求、处理接收内容的 API。例如，实现与 HTML 文档的 DOM 树进行交互。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>支持 HTTP 和 SOCKS 代理，可使用也可不使用认证。</li>\n<li>自动字符集检测。</li>\n<li>强大的 API，支持使用 XPath 查询从 HTML 文档的 DOM 树中抽取数据。</li>\n<li>自动 Cookie（或会话）支持。</li>\n</ul>\n<p>– <strong>官方文档</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//grablib.org/en/latest/\">https://grablib.org/en/latest/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/lorien/grab\">https://github.com/lorien/grab</a></strong></p>\n<h2 id=\"Java-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#Java-编写的开源-Web-爬虫\">※</a><strong>Java 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"10-Apache-Nutch\"><a class=\"header-anchor\" href=\"#10-Apache-Nutch\">※</a><strong>10. Apache Nutch</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>： 1743</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic1.zhimg.com/v2-3554a536f366655fc1cfb54ce52ce32c_r.jpg\" alt=\"\"></p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Apache Nutch 是一种高度可扩展、可伸缩的开源 Web 爬虫软件项目。</li>\n<li>如果要列出最好的开源 Web 爬虫列表，Apache Nutch 无疑金榜题名。</li>\n<li>作为一种用于数据挖掘的高度可扩展、可伸缩的开源代码 Web 数据抽取软件项目，Apache Nutch 得到了广泛的使用。</li>\n<li>Nutch 支持单机运行，但是在 Hadoop 集群上运行可最大化地发挥其强大能力。</li>\n<li>全球范围内很多数据分析人员和科研人员、应用开发人员和 Web 文本挖掘工程师都在使用 Apache Nutch。</li>\n<li>Apache Nutch 是一种 Java 编写的跨平台解决方案。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>默认情况下，爬取数据和分析数据是独立的过程。</li>\n<li>广泛支持多种文档格式，包括纯文本、HTML/XHTML+XML、XML、PDF、ZIP 等。</li>\n<li>使用 XPath 和命名空间实现映射。</li>\n<li>通过 Hadoop 支持分布式文件系统。</li>\n<li>链接图形式的数据库。</li>\n<li>支持 NTLM 认证。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//wiki.apache.org/nutch/\">https://wiki.apache.org/nutch/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//nutch.apache.org/\">http://nutch.apache.org/</a></strong></p>\n<h3 id=\"11-Heritrix\"><a class=\"header-anchor\" href=\"#11-Heritrix\">※</a><strong>11. Heritrix</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>： 1236</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>在使用 Java 编写的免费开源 Web 爬虫中，Heritrix 是其中一种得到广泛使用的工具。事实上，它是一种可扩展、Web 规模、存档质量（archival-quality）的 Web 爬取项目。</li>\n<li>Heritrix 是一种扩展能力和性能很好的解决方案，支持用户即刻爬取并归档一组网站。此外，它在设计上考虑了 robots.txt 禁止规则和 META 机器人标签。</li>\n<li>Heritrix 可运行在 Linux/Unix 和 Windows 系统上。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>HTTP 认证。</li>\n<li>NTLM 认证。</li>\n<li>链接抽取中的 XSL 转换。</li>\n<li>独立于搜索引擎。</li>\n<li>是一种成熟并稳定的平台。</li>\n<li>高度可配置。</li>\n<li>支持在任一机器上运行。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/internetarchive/heritrix3/wiki/Heritrix%25203.0%2520and%25203.1%2520User%2520Guide\">https://github.com/internetarchive/heritrix3/wiki/Heritrix%203.0%20and%203.1%20User%20Guide</a></strong></p>\n<p>– <strong>官方站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/internetarchive/heritrix3b\">https://github.com/internetarchive/heritrix3b</a></strong></p>\n<h3 id=\"12-ACHE-爬虫\"><a class=\"header-anchor\" href=\"#12-ACHE-爬虫\">※</a><strong>12. ACHE 爬虫</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>： 154</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p>![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='231' height='120'></svg>)</p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>ACHE 是一种专用于特定用途的 Web 爬虫。</li>\n<li>ACHE 爬取满足特定标准的 Web 页面。例如，属于特定领域并包含用户指定模式的页面。</li>\n<li>不同于通用爬虫，ACHE 使用页面分类器遴选特定领域中的相关和无关页面。</li>\n<li>页面分类器可以是基本的正则表达式（例如，匹配所有包含给定单词的页面），也可以基于机器学习的分类模型。ACHE 也可以自动学习如何对链接做优先处理，实现高效地定位相关内容，避免检索无关的页面内容。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>对固定网站列表的正常爬取。</li>\n<li>通过自动链接优先处理，发现并爬取新的相关网站。</li>\n<li>可配置不同类型的页面分类器（例如，机器学习、正则表达式等）。</li>\n<li>持续重新爬取站点，实现页面更新的发现。</li>\n<li>使用 ElasticSearch 对爬取页面做索引。</li>\n<li>实时搜索爬取页面的 Web 接口。</li>\n<li>用于监控爬虫的 REST API 和基于 Web 的用户接口。</li>\n<li>使用 TOR 代理爬取隐含服务。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//ache.readthedocs.io/en/latest/\">http://ache.readthedocs.io/en/latest/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/ViDA-NYU/ache\">https://github.com/ViDA-NYU/ache</a></strong></p>\n<h3 id=\"13-Crawler4j\"><a class=\"header-anchor\" href=\"#13-Crawler4j\">※</a><strong>13. Crawler4j</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>： 3039</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>crawler4j 是一种 Java 编写的开源 Web 爬虫，提供了爬取 Web 网站的基本接口。</li>\n<li>开发人员可以使用 crawler4j 在数分钟内建立一个多线程 Web 爬虫。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/yasserg/crawler4j\">https://github.com/yasserg/crawler4j</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/yasserg/crawler4j\">https://github.com/yasserg/crawler4j</a></strong></p>\n<h3 id=\"14-Gecco\"><a class=\"header-anchor\" href=\"#14-Gecco\">※</a><strong>14. Gecco</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>： 1245</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Gecco 是一种使用 Java 开发的轻量级 Web 爬虫，易于使用。</li>\n<li>Gecco 集成了 jsoup、httpclient、fastjson、spring、htmlunit、redission 等优秀框架。用户只需要配置一系列 jQuery 风格选择器，就能很快地建立一个爬虫。</li>\n<li>Gecco 框架具有优秀的扩展能力。框架基于一些开放式和封闭式设计原则，对改进封闭，对扩展开放。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>易于使用，使用 jQuery 风格选择器抽取元素。</li>\n<li>支持页面中的异步 Ajax 请求。</li>\n<li>支持页面 JavaScript 变量抽取。</li>\n<li>使用 Redis 实现分布式爬取（参见 gecco-redis 文档）。</li>\n<li>支持使用 Spring 开发业务逻辑（参见 gecco-spring 文档）。</li>\n<li>支持 htmlunit 扩展（参见 gecco-htmlunit 文档）。</li>\n<li>支持多种扩展机制。</li>\n<li>支持下载 UserAgent 的随机选择。</li>\n<li>支持下载代理服务器的随机选取。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/xtuhcy/gecco\">https://github.com/xtuhcy/gecco</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/xtuhcy/gecco\">https://github.com/xtuhcy/gecco</a></strong></p>\n<h3 id=\"15-BUbiNG\"><a class=\"header-anchor\" href=\"#15-BUbiNG\">※</a><strong>15. BUbiNG</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>：24</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>BUbiNG 令人惊喜，它可称为下一代的开源 Web 爬虫。BUbiNG 是一种 Java 开发的完全分布式爬虫（无需中央协调），每秒可爬取数千个网页，并支持采集大规模数据集。</li>\n<li>BUbiNG 的分布式是基于高速协议实现的，因此可以获得非常高的通量。</li>\n<li>BUbiNG 提供对海量数据的大规模爬取。它完全可配置、易于扩展，并可集成垃圾信息检测。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>高度并行。</li>\n<li>完全分布式。</li>\n<li>使用 JAI4J。JAI4J 是一种基于 JGroups 实现的瘦功能层，实现任务指派。</li>\n<li>（当前）使用剥离网页的指纹，检测近似的重复内容。</li>\n<li>快速。</li>\n<li>大规模爬取。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//law.di.unimi.it/software/bubing-docs/index.html\">http://law.di.unimi.it/software/bubing-docs/index.html</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//law.di.unimi.it/software.php%23bubing\">http://law.di.unimi.it/software.php#bubing</a></strong></p>\n<h3 id=\"16-Narconex\"><a class=\"header-anchor\" href=\"#16-Narconex\">※</a><strong>16. Narconex</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：Java</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic4.zhimg.com/v2-47ae5be7edc6c681a379c405899abc2f_r.jpg\" alt=\"\"></p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>对于寻求可满足企业级需求的开源 Web 爬虫的用户而言，Narconex 是一种很好的工具。</li>\n<li>Norconex 支持用户爬取任何 Web 内容。用户可以独立运行这种全功能数据采集器，或是将其集成在自己的应用中。</li>\n<li>支持所有操作系统。可在具有一般容量的单体服务器上爬取数百万信息。此外，Narconex 提供多种内容和元数据操作特性，还可以抽取页面中特定的图像。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>多线程。</li>\n<li>支持按各种计划任务，抽取不同时间段的数据。</li>\n<li>从 HTML、Word、PDF 等多种文件格式中抽取文本内容。</li>\n<li>抽取文档相关的元数据。</li>\n<li>支持抽取使用用 JavaScript 渲染的页面。</li>\n<li>检测语言。</li>\n<li>支持翻译。</li>\n<li>可配置爬取速度。</li>\n<li>可检测发生修改或已被删除的文档。</li>\n<li>支持使用外部命令分析或操作文档。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//www.norconex.com/collectors/collector-http/getting-started\">http://www.norconex.com/collectors/collector-http/getting-started</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//www.norconex.com/collectors/collector-http/\">http://www.norconex.com/collectors/collector-http/</a></strong></p>\n<h3 id=\"17-WebSPHINX\"><a class=\"header-anchor\" href=\"#17-WebSPHINX\">※</a><strong>17. WebSPHINX</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li>当前尚不提供官方支持。</li>\n</ul>\n<p>![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='296' height='225'></svg>)</p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>WebSphinix 是一种非常易于使用的可定制 Web 爬虫。它设计用于高级 Web 用户和 Java 编程人员，支持他们自动爬取小部分 Web。</li>\n<li>WebSphinix 数据抽取解决方案也提供了一种全面的 Java 类库和交互式软件开发环境。WebSphinix 包括两部分：爬虫基准测试（Crawler Workbench），WebSPHINX 类库。</li>\n<li>爬虫基准测试提供了很好的用户图形接口，支持用户配置并控制定制的 Web 爬虫。</li>\n<li>WebSPHINX 类库为使用 Java 编写 Web 爬虫提供支持。</li>\n<li>WebSphinix 支持运行在 Windows、Linux、Mac 和 Android IOS 上。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>以图的方式可视化 Web 页面采集。</li>\n<li>将多个页面组合为单个文档，以供查看和打印。</li>\n<li>支持抽取所有满足设定模式的文本。</li>\n<li>支持 HTML 解析。</li>\n<li>支持 robot.txt 禁止标准。</li>\n<li>通用 HTML 转换。</li>\n<li>多线程 Web 页面检索。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/~rcm/websphinx/doc/index.html\">https://www.cs.cmu.edu/~rcm/websphinx/doc/index.html</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/~rcm/websphinx/%23about\">https://www.cs.cmu.edu/~rcm/websphinx/#about</a></strong></p>\n<h3 id=\"18-Spiderman\"><a class=\"header-anchor\" href=\"#18-Spiderman\">※</a><strong>18. Spiderman</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>： 2400</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Spiderman 是一种 Java 开源 Web 数据抽取工具。它采集特定的 Web 页面，并从中抽取有用数据。</li>\n<li>Spiderman 主要使用 XPath 和正则表达式等技术抽取实际数据。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>更高的性能。</li>\n<li>持久化集合状态。</li>\n<li>分布式。</li>\n<li>支持 JavaScript。</li>\n<li>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//gitee.com/l-weiwei/spiderman\">https://gitee.com/l-weiwei/spiderman</a></strong></li>\n</ul>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//gitee.com/l-weiwei/spiderman\">https://gitee.com/l-weiwei/spiderman</a></strong></p>\n<h3 id=\"19-WebCollector\"><a class=\"header-anchor\" href=\"#19-WebCollector\">※</a><strong>19. WebCollector :</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>： 1986</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>WebCollector 是一种基于 Java 的开源 Web 爬虫框架。</li>\n<li>它为实现 Web 爬取功能提供了一下基本的接口。用户可以使用它在五分钟内建立起一个多线程爬虫。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>快速。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/CrawlScript/WebCollector\">https://github.com/CrawlScript/WebCollector</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/CrawlScript/WebCollector\">https://github.com/CrawlScript/WebCollector</a></strong></p>\n<h3 id=\"20-Webmagic\"><a class=\"header-anchor\" href=\"#20-Webmagic\">※</a><strong>20. Webmagic</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>： 6891</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic3.zhimg.com/v2-c0b0e6efba453137639551e5d11a1542_r.jpg\" alt=\"\"></p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>WebMagic 是一种可扩展的爬虫框架。</li>\n<li>WebMagic 涵盖了爬虫的整个生命周期，包括下载、URL 管理、内容抽取和持久化。</li>\n<li>可用于简化一些特定爬虫的开发。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>高度灵活的简单内核。</li>\n<li>提供实现 HTML 抽取的简单 API。</li>\n<li>使用 POJO 标注定制爬虫，无需配置。</li>\n<li>支持多线程和分布式。</li>\n<li>易于集成。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//webmagic.io/docs/en/\">http://webmagic.io/docs/en/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/code4craft/webmagic\">https://github.com/code4craft/webmagic</a></strong></p>\n<h3 id=\"21-StormCrawler\"><a class=\"header-anchor\" href=\"#21-StormCrawler\">※</a><strong>21. StormCrawler</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Java</li>\n<li><strong>GitHub Star 数</strong>：437</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic4.zhimg.com/v2-4f40d9113c161884a19fd41756e93507_r.jpg\" alt=\"\"></p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>StormCrawler 是一种基于 Apache Storm 构架分布式 Web 爬虫的开源 SDK。</li>\n<li>StormCrawler 为开发人员构建爬虫提供了软件库和一系列资源。</li>\n<li>StormCrawler 完全适用于以数据流提供需获取和解析的 URL 的情况，也非常适用于大规模递归性爬取，尤其是需要低延迟的情况。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>可扩展。</li>\n<li>有弹性。</li>\n<li>低延迟。</li>\n<li>易于扩展。</li>\n<li>运行良好且高效。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//stormcrawler.net/docs/api/\">http://stormcrawler.net/docs/api/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//stormcrawler.net/\">http://stormcrawler.net/</a></strong></p>\n<h2 id=\"JavaScript-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#JavaScript-编写的开源-Web-爬虫\">※</a><strong>JavaScript 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"22-NodeCrawler\"><a class=\"header-anchor\" href=\"#22-NodeCrawler\">※</a><strong>22. NodeCrawler</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： JavaScript</li>\n<li><strong>GitHub Star 数</strong>： 3999</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic3.zhimg.com/v2-ddebe1bae52d5b0f56687f79d6bcfbf2_r.jpg\" alt=\"\"></p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>NodeCrawler 是一种广为使用的 Web 爬虫，它基于 NodeJS 实现，具有非常快的爬取速度。</li>\n<li>Nodecrawler 非常适用于偏爱使用 JavaScript 编程或者致力于 JavaScript 项目的开发人员。其安装也非常简单。</li>\n<li>JSDOM 和 Cheerio（用于 HTML 解析）实现服务器端渲染。其中，JSDOM 更为稳定。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>使用 Cheerio（默认）或 JSDOM 实现服务端 DOM 和自动 jQuery 插入。</li>\n<li>可配置池子规模和重试次数。</li>\n<li>控制爬取率限制。</li>\n<li>请求的优先队列。</li>\n<li>支持 forceUTF8 模式，使得爬虫可以检测并转换字符集。</li>\n<li>与 4.x 乃至更新版本兼容。</li>\n</ul>\n<p>– <strong>官方文档</strong>：<strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/bda-research/node-crawler\">https://github.com/bda-research/node-crawler</a></strong></p>\n<p>– <strong>官方网站</strong>：<strong><a href=\"https://link.zhihu.com/?target=http%3A//nodecrawler.org/\">http://nodecrawler.org/</a></strong></p>\n<h3 id=\"23-Simplecrawler\"><a class=\"header-anchor\" href=\"#23-Simplecrawler\">※</a><strong>23. Simplecrawler</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： JavaScript</li>\n<li><strong>GitHub Star 数</strong>：1764</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Simplecrawler 设计提供基本的、灵活且稳定的网站爬取 API。</li>\n<li>Simplecrawler 在实现上考虑了针对特大型 Web 站点的归档、分析和搜索。它可爬取上百万页面，并毫无问题地向磁盘写入数十 GB 数据。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>提供了用于自动检测链接资源的一些简单逻辑，用户可自行替换和添加。</li>\n<li>自动请求任何 robots.txt 禁止规则。</li>\n<li>具有灵活的队列系统，可在磁盘上冻结和解冻。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/simplecrawler/simplecrawler\">https://github.com/simplecrawler/simplecrawler</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.npmjs.com/package/simplecrawler\">https://www.npmjs.com/package/simplecrawler</a></strong></p>\n<h3 id=\"24-Js-crawler\"><a class=\"header-anchor\" href=\"#24-Js-crawler\">※</a><strong>24. Js-crawler :</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： JavaScript</li>\n<li><strong>GitHub Star 数</strong>： 167</li>\n<li><strong>官方支持链接)</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>使用 NodeJS 实现的 Web 爬虫，支持 HTTP 和 HTTPS</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/antivanov/js-crawler\">https://github.com/antivanov/js-crawler</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/antivanov/js-crawler\">https://github.com/antivanov/js-crawler</a></strong></p>\n<h3 id=\"25-Webster\"><a class=\"header-anchor\" href=\"#25-Webster\">※</a><strong>25. Webster</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： JavaScript</li>\n<li><strong>GitHub Star 数</strong>： 201</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Webster 是一种使用 NodeJS 编写的可靠 Web 爬取和采集框架，用于爬取 Web 站点并从页面中抽取结构化数据。</li>\n<li>与其他爬取框架的不同之处在于，Webster 可以抓取浏览器客户端的 JavaScript 和 Ajax 请求呈现的内容。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//webster.zhuyingda.com/\">http://webster.zhuyingda.com/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/zhuyingda/webster\">https://github.com/zhuyingda/webster</a></strong></p>\n<h3 id=\"26-Node-osmosis\"><a class=\"header-anchor\" href=\"#26-Node-osmosis\">※</a><strong>26. Node-osmosis</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：JavaScript</li>\n<li><strong>GitHub Star 数</strong>： 3630</li>\n<li>**<strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/rchipka/node-osmosis/issues\">官方支持链接</a></strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<p>* 一种使用 NodeJS 实现的 HTML/XML 解析器和 Web 爬虫。</p>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>使用原生 libxml 的 C 绑定。</li>\n<li>干净的 Promise 类接口。</li>\n<li>支持 CSS 3.0 和 XPath 1.0 选择器的混合。</li>\n<li><strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/jquery/sizzle/wiki%23other-selectors-and-conventions\">Sizzle 选择器</a></strong>、<strong><a href=\"https://link.zhihu.com/?target=http%3A//mootools.net/core/docs/1.6.0/Slick/Slick\">Slick 选择器</a><strong>以及</strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/rchipka/node-osmosis/blob/master/docs/Selectors.md\">更多</a></strong>。</li>\n<li>不具有像 jQuery、cheerio 或 jsdom 那样的大型依赖。</li>\n<li>构成深度和复杂的数据结构。</li>\n<li>HTML 解析器特性：</li>\n<li>快速解析；</li>\n<li>高速搜索；</li>\n<li>内存占用小。</li>\n<li>HTML DOM 特性：</li>\n<li>加载和搜索 ajax 内容；</li>\n<li>DOM 交互和事件；</li>\n<li>执行嵌入和远程脚本；</li>\n<li>在 DOM 中执行代码。</li>\n<li>HTTP 请求特性：</li>\n<li>日志记录 URL，重定向和错误；</li>\n<li>Cookie 的 jar 包，以及自定义 Cookie/头部/用户代理；</li>\n<li>登录/表单提交、会话 Cookie，基本的认证；</li>\n<li>单代理、多代理，处理代理失败情况；</li>\n<li>限制重试和重定向。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//rchipka.github.io/node-osmosis/global.html\">https://rchipka.github.io/node-osmosis/global.html</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.npmjs.com/package/osmosis\">https://www.npmjs.com/package/osmosis</a></strong></p>\n<h3 id=\"27-Supercrawler\"><a class=\"header-anchor\" href=\"#27-Supercrawler\">※</a><strong>27. Supercrawler</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：JavaScript</li>\n<li><strong>GitHub Star 数</strong>： 4341</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Supercrawler 是一种使用 NodeJS 实现的 Web 爬虫，在设计上支持高度可配置和易用性。</li>\n<li>一旦成功爬取一个网页（可以是图像、文本文档或其他任何文件），Supercrawler 将会触发用户自定义的内容类型（content-type）处理器，处理页面解析、保存数据以及其它一些用户定义的功能。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>链接检测：Supercrawler 会解析所爬取的 HTML 文档，识别其中链接并添加到队列中。</li>\n<li>机器人解析：在爬取前 Supercrawler 会请求 robots.txt 并检查其中的禁止规则。它也可识别站点地图。</li>\n<li>站点地图解析：Supercrawler 可以从 XML 站点地图文件中读取链接，并将链接添加到队列中。</li>\n<li>并发限制：Supercrawler 可限制在任一时间发出的请求数。</li>\n<li>速率限制：Supercrawler 可添加请求的时延，以免对服务器产生轰炸。</li>\n<li>指数补偿（Exponential backoff）重试：Supercrawler 将依此在一小时、两小时、四小时乃至更多时间后重试请求。要使用该特性，爬取队列必须使用数据库或 Redis 作为后端。</li>\n<li>主机名均衡：Supercrawler 可在不同主机名间平均分割请求量。要使用该特性，爬取队列必须以 Redis 为后端。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/brendonboshell/supercrawler\">https://github.com/brendonboshell/supercrawler</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/brendonboshell/supercrawler\">https://github.com/brendonboshell/supercrawler</a></strong></p>\n<h3 id=\"28-Web-scraper-的-Chrome-扩展\"><a class=\"header-anchor\" href=\"#28-Web-scraper-的-Chrome-扩展\">※</a><strong>28. Web scraper 的 Chrome 扩展</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：JavaScript</li>\n<li><strong>GitHub Star 数</strong>： 775</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Web Scraper 是一种 Chrome 浏览器扩展，构建用于从 Web 页面抽取数据。</li>\n<li>用户可以使用该扩展创建计划（站点地图），定义如何遍历一个 Web 网站，以及如何从中抽取数据。</li>\n<li>Web Scraper 使用站点地图相应地遍历网站，并从中抽取数据。</li>\n<li>支持以 CSV 格式导出所抽取的数据。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>抽取多个页面。</li>\n<li>站点地图和抽取的数据存储在浏览器的本地存储，也可存储在 CouchDB 中。</li>\n<li>多种数据类型选取。</li>\n<li>支持从动态网页（JavaScript+AJAX）抽取数据。</li>\n<li>浏览抽取的数据。</li>\n<li>以 CSV 格式导出抽取的数据。</li>\n<li>导入、导出站点地图。</li>\n<li>只依赖于 Chrome 浏览器。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.webscraper.io/documentation\">https://www.webscraper.io/documentation</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.webscraper.io/\">https://www.webscraper.io</a></strong></p>\n<h3 id=\"29-Headless-Chrome-爬虫\"><a class=\"header-anchor\" href=\"#29-Headless-Chrome-爬虫\">※</a><strong>29. Headless Chrome 爬虫</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：JavaScript</li>\n<li><strong>GitHub Star 数</strong>： 3256</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic2.zhimg.com/v2-59980cb14aeb8114d240a806a8a5caf5_r.jpg\" alt=\"\"></p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>使用基本 HTML 文件请求的爬虫，通常速度很快。但这样的爬虫往往会抽取到空白内容，尤其是在爬取使用 AngularJS、React 和 Vue.js 等现代前端框架构建的网站时。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>分布式爬取。</li>\n<li>可配置并发、延迟和重试。</li>\n<li>支持深度优先搜索和广度优先搜索算法。</li>\n<li>支持插拔缓存存储，例如 Redis。</li>\n<li>支持导出 CSV 和 JSON。</li>\n<li>在达到最大请求时暂停爬取，并可在任一时刻恢复。</li>\n<li>自动插入用于抽取的 jQuery。</li>\n<li>保存屏幕截图，用于证实爬取过程。</li>\n<li>模拟设备和用户代理。</li>\n<li>具有优先队列，可提高爬取效率。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/yujiosaka/headless-chrome-crawler/blob/master/docs/API.md\">https://github.com/yujiosaka/headless-chrome-crawler/blob/master/docs/API.md</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/yujiosaka/headless-chrome-crawler\">https://github.com/yujiosaka/headless-chrome-crawler</a></strong></p>\n<h3 id=\"30-X-ray\"><a class=\"header-anchor\" href=\"#30-X-ray\">※</a><strong>30. X-ray</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：JavaScript</li>\n<li><strong>GitHub Star 数</strong>： 4464</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><img src=\"https://pic3.zhimg.com/v2-898fd51a134e7ea31554fca0ff89bfda_r.jpg\" alt=\"\"></p>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>模式灵活：支持字符串、数组、对象以及嵌套对象结构。模式并非绑定于所抽取的页面结构，支持用户获取选定结构中的数据。</li>\n<li>可组合（Composable）：API 是完全可组合的，赋予用户抽取每个页面的极大灵活性。</li>\n<li>分页支持：爬取页面在 Web 站点上的所有分页。X-ray 还支持请求延迟和分页限制，并支持将爬取页面导入到单个文件中。这样一旦单个页面产生错误，用户不会失去所有已爬取的数据。</li>\n<li>爬虫支持：从一个页面开始，轻易跳转另一个页面。页面跳转是可预测的，按深度优先爬取所有页面。</li>\n<li>负责任的爬取：X-ray 支持并发、限制、延迟、超时和限制，实现负责任地爬取任何页面。</li>\n<li>可插拔驱动：可按用户需求置换不同的爬虫。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/matthewmueller/x-ray\">https://github.com/matthewmueller/x-ray</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.npmjs.com/package/x-ray-scraper\">https://www.npmjs.com/package/x-ray-scraper</a></strong></p>\n<h2 id=\"C-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#C-编写的开源-Web-爬虫\">※</a><strong>C 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"31-Httrack\"><a class=\"header-anchor\" href=\"#31-Httrack\">※</a><strong>31. Httrack</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：C</li>\n<li><strong>GitHub Star 数</strong>： 747</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p>![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='383' height='33'></svg>)</p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>HTTracks 是一项免费（GPL、Libre/自由软件）且易于使用的离线浏览器功能。</li>\n<li>支持用户将 Web 站点下载到本地目录，递归构建全部目录，以及获取 HTML、图像和其它文件到本地计算机。</li>\n<li>HTTrack 会维持原站点的相对链接结构。用户可以用浏览器打开本地的“镜像”页面，并逐个链接浏览，与在线浏览无异。</li>\n<li>HTTrack 也支持对已有镜像站点的更新，以及从中断点恢复下载。</li>\n<li>HTTrack 高度可配置，并提供帮助文档。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>多语言窗口，提供对 Linux/UNIX 的接口。</li>\n<li>镜像单个站点，或是一并镜像多个站点。</li>\n<li>支持按文件类型、链接位置、结构深度、文件大小过滤，接受或拒绝站点或文件名。</li>\n<li>支持代理，可最大化速度，并可选认证。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//www.httrack.com/html/index.html\">http://www.httrack.com/html/index.html</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//www.httrack.com/\">http://www.httrack.com/</a></strong></p>\n<h3 id=\"32-GNU-Wget\"><a class=\"header-anchor\" href=\"#32-GNU-Wget\">※</a><strong>32. GNU Wget</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：C</li>\n<li><strong>GitHub Star 数</strong>： 22</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p>![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='139' height='136'></svg>)</p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>GNU Wget 是一种免费软件包，它使用 HTTP、HTTPS、FTP、FTPS 等广为使用的互联网协议检索文件。</li>\n<li>Wget 是一种非交互式命令行工具，易于从脚本、Cron 任务、不具有 X 窗口支持的终端等处调用。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>使用 REST 和 RANGE 支持从中断处恢复下载。</li>\n<li>基于 NLS 的消息文件，可使用多种语言。</li>\n<li>可运行于大多数类 UNIX 操作系统上，也支持 Windows.</li>\n<li>支持 HTTP 代理。</li>\n<li>支持 HTTP Cookie。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.gnu.org/software/wget/manual/\">https://www.gnu.org/software/wget/manual/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//www.gnu.org/software/wget/\">https://www.gnu.org/software/wget/</a></strong></p>\n<h2 id=\"C-编写的开源-Web-爬虫-v2\"><a class=\"header-anchor\" href=\"#C-编写的开源-Web-爬虫-v2\">※</a><strong>C++编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"33-gigablast\"><a class=\"header-anchor\" href=\"#33-gigablast\">※</a><strong>33. gigablast</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：C++</li>\n<li><strong>GitHub Star 数</strong>： 912</li>\n<li>**<strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/gigablast/open-source-search-engine/issues\">官方支持链接</a></strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Gigablast 是一种开源的 Web 和企业搜索引擎，也是一种爬虫。</li>\n<li>Gigablast 是自身维护数十亿页面检索索引的数家美国搜索引擎之一。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>大规模。</li>\n<li>高性能。</li>\n<li>实时信息检索技术。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//www.gigablast.com/api.html\">http://www.gigablast.com/api.html</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//www.gigablast.com/\">http://www.gigablast.com/</a></strong></p>\n<h2 id=\"C-编写的开源-Web-爬虫-v3\"><a class=\"header-anchor\" href=\"#C-编写的开源-Web-爬虫-v3\">※</a><strong>C#编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"34-http-Arachnode-net\"><a class=\"header-anchor\" href=\"#34-http-Arachnode-net\">※</a><strong>34. <a href=\"https://link.zhihu.com/?target=http%3A//Arachnode.net\">http://Arachnode.net</a></strong></h3>\n<ul>\n<li><strong>实现语言</strong>：C#</li>\n<li><strong>GitHub Star 数</strong>： 9</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li><a href=\"https://link.zhihu.com/?target=http%3A//Arachnode.net\">http://Arachnode.net</a> 适用于寻求开源 Web 爬虫的 C#开发人员。</li>\n<li><a href=\"https://link.zhihu.com/?target=http%3A//Arachnode.net\">http://Arachnode.net</a> 软件类库从因特网下载内容、对内容做索引，并对过程做定制。</li>\n<li>用户可使用该工具做个人内容聚合，也可用于将下载的内容抽取、采集和解析为多个表单。</li>\n<li><a href=\"https://link.zhihu.com/?target=http%3A//Arachnode.net\">http://Arachnode.net</a> 索引所发现的内容，并存储在 <a href=\"https://link.zhihu.com/?target=http%3A//Lucene.NET\">http://Lucene.NET</a> 索引中。</li>\n<li><a href=\"https://link.zhihu.com/?target=http%3A//Arachnode.net\">http://Arachnode.net</a> 非常适用于文本挖掘，也适用于学习高级爬取技术。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>可配置规则和行为。</li>\n<li>集成 <a href=\"https://link.zhihu.com/?target=http%3A//Lucene.NET\">http://Lucene.NET</a>。</li>\n<li>支持 SQL Server 和全文本索引。</li>\n<li>支持对.DOC/.PDF/.PPT/.XLS 等文件类型的索引。</li>\n<li>支持将 HTML 转化为 XML 和 XHTML。</li>\n<li>支持全部 JavaScript/AJAX 功能。</li>\n<li>支持多线程和节流(Throttling)。</li>\n<li>行为适当（Respectful）的爬取。</li>\n<li>分析服务。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//documentation.arachnode.net/index.html\">https://documentation.arachnode.net/index.html</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//arachnode.net/\">http://arachnode.net/</a></strong></p>\n<h3 id=\"35-Abot\"><a class=\"header-anchor\" href=\"#35-Abot\">※</a><strong>35. Abot</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：C#</li>\n<li><strong>GitHub Star 数</strong>： 1392</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Abot 是一种 C#实现的开源 Web 爬虫，主要侧重于速度和灵活性。</li>\n<li>Abot 在实现中考虑了底层技术细节，包括多线程、HTTP 请求、调度、链接解析等。</li>\n<li>用户只需注册事件，就可以处理分页数据。</li>\n<li>支持用户插入自己的核心接口实现，实现对整个爬取过程的完全控制。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>高速！</li>\n<li>易于定制（可插拔架构，支持用户定义爬取内容和方式）。</li>\n<li>经过大量的单元测试（高代码覆盖率）。</li>\n<li>非常轻量级（并非过度工程化）。</li>\n<li>无过程之外的依赖，例如对数据库、所安装服务等的依赖。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/sjdirect/abot\">https://github.com/sjdirect/abot</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/sjdirect/abot\">https://github.com/sjdirect/abot</a></strong></p>\n<h3 id=\"36-Hawk\"><a class=\"header-anchor\" href=\"#36-Hawk\">※</a><strong>36. Hawk</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：C#</li>\n<li><strong>GitHub Star 数</strong>： 1875</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>HAWK 无需用户做任何编程，提供图形可视化数据获取和清理工具，并以 GPL 协议开源。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>无需编程，即可实现对 Web 内容的智能分析。</li>\n<li>所见即所得（WYSIWYG），可视化拉拽，支持对数据转换和过滤等的快速处理。</li>\n<li>支持从多种数据库和文件中导入和导出。</li>\n<li>任务可保存并可重用。</li>\n<li>尤其适用于爬取和数据清理，但其功能并不仅局限于此。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/ferventdesert/Hawk\">https://github.com/ferventdesert/Hawk</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//ferventdesert.github.io/Hawk/\">https://ferventdesert.github.io/Hawk/</a></strong></p>\n<h3 id=\"37-SkyScraper\"><a class=\"header-anchor\" href=\"#37-SkyScraper\">※</a><strong>37. SkyScraper</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：C#</li>\n<li><strong>GitHub Star 数</strong>： 39</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>一种异步 Web 获取和爬虫，使用了 async/await 和响应式扩展。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/JonCanning/SkyScraper\">https://github.com/JonCanning/SkyScraper</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/JonCanning/SkyScraper\">https://github.com/JonCanning/SkyScraper</a></strong></p>\n<h2 id=\"NET-编写的-Web-爬虫\"><a class=\"header-anchor\" href=\"#NET-编写的-Web-爬虫\">※</a><strong>.NET 编写的 Web 爬虫</strong></h2>\n<h3 id=\"38-DotnetSpider\"><a class=\"header-anchor\" href=\"#38-DotnetSpider\">※</a><strong>38. DotnetSpider</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：.NET</li>\n<li><strong>GitHub Star 数</strong>： 1382</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>DotnetSpider <a href=\"http://xn--4gq59a111cewn2of.NET\">是一种使用.NET</a> Standard 实现的 Web 爬取软件库，类似于 WebMagic 和 Scrapy。</li>\n<li><a href=\"http://xn--4gq6mt54aiwkvhs2lg935b.NET\">它是一种适用于.NET</a> 的轻量级、高效和高速的高层 Web 爬取和获取框架。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/dotnetcore/DotnetSpider/wiki\">https://github.com/dotnetcore/DotnetSpider/wiki</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/dotnetcore/DotnetSpider\">https://github.com/dotnetcore/DotnetSpider</a></strong></p>\n<h2 id=\"PHP-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#PHP-编写的开源-Web-爬虫\">※</a><strong>PHP 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"39-Goutte\"><a class=\"header-anchor\" href=\"#39-Goutte\">※</a><strong>39. Goutte</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：PHP</li>\n<li><strong>GitHub Star 数</strong>： 6574</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Goutte 是一种 PHP 实现的屏幕抓取和 Web 爬取软件库。</li>\n<li>Goutte 为爬取 Web 站点和从 HTML/XML 响应中抽取数据提供了很好的 API。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//goutte.readthedocs.io/en/latest/\">https://goutte.readthedocs.io/en/latest/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/FriendsOfPHP/Goutte\">https://github.com/FriendsOfPHP/Goutte</a></strong></p>\n<h3 id=\"40-Dom-crawler\"><a class=\"header-anchor\" href=\"#40-Dom-crawler\">※</a><strong>40. Dom-crawler</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：PHP</li>\n<li><strong>GitHub Star 数</strong>： 1340</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>DomCrawler 组件简化了对 HTML 和 XML 文档的 DOM 浏览。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//symfony.com/doc/current/components/dom_crawler.html\">https://symfony.com/doc/current/components/dom_crawler.html</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/symfony/dom-crawler\">https://github.com/symfony/dom-crawler</a></strong></p>\n<h3 id=\"41-Pspider\"><a class=\"header-anchor\" href=\"#41-Pspider\">※</a><strong>41. Pspider</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：PHP</li>\n<li><strong>GitHub Star 数</strong>： 249</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Pspider 是最近完全使用 PHP 实现的一种并行爬取框架，它基于 hightman/httpclient 组件。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/hightman/pspider\">https://github.com/hightman/pspider</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/hightman/pspider\">https://github.com/hightman/pspider</a></strong></p>\n<h3 id=\"42-Php-spider\"><a class=\"header-anchor\" href=\"#42-Php-spider\">※</a><strong>42. Php-spider</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：PHP</li>\n<li><strong>GitHub Star 数</strong>： 1023</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>一种可配置、可扩展的 Web 爬虫。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>可限制爬取深度、队列大小和最大下载数。</li>\n<li>支持基于 XPath、CSS 选择器或普通（Plain old）PHP 添加自定义的 URI 发现逻辑。</li>\n<li>提供了一系列有用的 URI 过滤器，例如域限制等。</li>\n<li>收集爬取统计信息，用于形成报告。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/mvdbos/php-spider\">https://github.com/mvdbos/php-spider</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/mvdbos/php-spider\">https://github.com/mvdbos/php-spider</a></strong></p>\n<h3 id=\"43-Spatie-Crawler\"><a class=\"header-anchor\" href=\"#43-Spatie-Crawler\">※</a><strong>43. Spatie / Crawler</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：PHP</li>\n<li><strong>GitHub Star 数</strong>： 740</li>\n<li>**<strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/spatie/crawler/issues\">官方支持链接</a></strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>该软件包提供了从 Web 站点爬取链接的类。在实现的底层机制上，使用了 GuzzleHttp/Promise 并发爬取多个 URL。</li>\n<li>该爬虫支持执行 JavaScript，可以爬取使用 JavaScript 渲染的站点。从实现的底层机制看，该特性使用了 Chrome 和 Puppeteer。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/spatie/crawler\">https://github.com/spatie/crawler</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/spatie/crawler\">https://github.com/spatie/crawler</a></strong></p>\n<h2 id=\"Ruby-实现的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#Ruby-实现的开源-Web-爬虫\">※</a><strong>Ruby 实现的开源 Web 爬虫</strong></h2>\n<h3 id=\"44-Mechanize\"><a class=\"header-anchor\" href=\"#44-Mechanize\">※</a><strong>44. Mechanize</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：Ruby</li>\n<li><strong>GitHub Star 数</strong>： 3728</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Mechanize 软件库用于实现于 Web 站点的自动交互。</li>\n<li>Mechanize 自动按重定向存储并发送 Cookie。可以根据链接提交表单，支持填写和提交表单域。</li>\n<li>Mechanize 也可跟踪用户访问过站点的历史记录。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//docs.seattlerb.org/mechanize/\">http://docs.seattlerb.org/mechanize/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/sparklemotion/mechanize\">https://github.com/sparklemotion/mechanize</a></strong></p>\n<h2 id=\"GO-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#GO-编写的开源-Web-爬虫\">※</a><strong>GO 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"45-Colly\"><a class=\"header-anchor\" href=\"#45-Colly\">※</a><strong>45. Colly</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：Go</li>\n<li><strong>GitHub Star 数</strong>： 5439</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p>![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='381' height='199'></svg>)</p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>为 Go 爱好者提供了一种快速且适用的爬取框架。</li>\n<li>Colly 提供了非常清晰的接口，可用于编写任何类型的爬虫和数据获取工具。</li>\n<li>Colly 使得用户可以轻易地从站点抽取结构化数据。这些数据适用于大范围的应用，例如数据挖掘、数据处理和归档。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>清晰的 API。</li>\n<li>高速（支持单核每秒处理一千次以上的请求）。</li>\n<li>按域管理请求延迟和最大并发。</li>\n<li>自动 Cookie 和会话管理。</li>\n<li>同步/异步/并行爬取。</li>\n<li>支持缓存。</li>\n<li>对非 unicode 响应的自动编码。</li>\n<li>支持 robots.txt 禁止规则。</li>\n<li>分布式爬取。</li>\n<li>可通过环境变量配置。</li>\n<li>支持扩展。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//go-colly.org/docs/\">http://go-colly.org/docs/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//go-colly.org/\">http://go-colly.org/</a></strong></p>\n<h3 id=\"46-Gopa\"><a class=\"header-anchor\" href=\"#46-Gopa\">※</a><strong>46. Gopa</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：Go</li>\n<li><strong>GitHub Star 数</strong>： 169</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>轻量级，低资源占用，小于 100MB 的内存需求。</li>\n<li>易于部署，无需任何运行时和依赖关系。</li>\n<li>易于使用，不需要用户具有任何编程和脚本技能，提供开箱即可用特性。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/infinitbyte/gopa\">https://github.com/infinitbyte/gopa</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/infinitbyte/gopa\">https://github.com/infinitbyte/gopa</a></strong></p>\n<h3 id=\"47-Pholcus\"><a class=\"header-anchor\" href=\"#47-Pholcus\">※</a><strong>47. Pholcus</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：Go</li>\n<li><strong>GitHub Star 数</strong>： 4341</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p>![](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='92' height='92'></svg>)</p>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Pholcus 是一种完全使用 Go 语言实现的高并发性、重量级爬虫软件。</li>\n<li>它针对因特网数据采集，为只具有基本 Go 或 JavaScript 编程基础的用户提供了一种只需要关注自定义功能的特性。</li>\n<li>规则简单灵活，并发批处理任务，提供丰富的输出方式，包括 MySQL、MongoDB、Kafka、CSV、Exvel 等。</li>\n<li>用户共享了大量的演示。此外，Pholcus 支持两种水平和垂直爬取模式，支持模拟登陆、暂停任务、取消任务等一系列高级特性。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>一种强大的爬取工具。</li>\n<li>支持三种运行模式：单机、服务器和客户。</li>\n<li>提供三种操作接口：Web、GUI 和命令行。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//pholcus.gitbooks.io/docs/\">https://pholcus.gitbooks.io/docs/</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/henrylee2cn/pholcus\">https://github.com/henrylee2cn/pholcus</a></strong></p>\n<h2 id=\"R-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#R-编写的开源-Web-爬虫\">※</a><strong>R 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"48-Rvest\"><a class=\"header-anchor\" href=\"#48-Rvest\">※</a><strong>48. Rvest</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：R</li>\n<li><strong>GitHub Star 数</strong>： 969</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Rvest 为用户从 Web 页面抽取信息提供帮助。它在设计上使用了 magrittr 软件包，易于表达通用 Web 抽取。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//cran.r-project.org/web/packages/rvest/rvest.pdf\">https://cran.r-project.org/web/packages/rvest/rvest.pdf</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/hadley/rvest\">https://github.com/hadley/rvest</a></strong></p>\n<h2 id=\"Scala-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#Scala-编写的开源-Web-爬虫\">※</a><strong>Scala 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"49-Sparkler\"><a class=\"header-anchor\" href=\"#49-Sparkler\">※</a><strong>49. Sparkler</strong></h3>\n<ul>\n<li><strong>实现语言</strong>： Scala</li>\n<li><strong>GitHub Star 数</strong>： 198</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Web 爬虫是一种机器人程序，它从 Web 网站采集资源，用于构建搜索引擎、知识库等应用。</li>\n<li>Sparkler（“Spark-Crawler”的缩写）是一种新型的 Web 爬虫，它通过整合 Spark、Kafka、Lucene/Solr、Tika、pf4j 等多种 Apache 项目，使用了分布式计算和信息检索领域的最新进展。</li>\n</ul>\n<p><strong>特性</strong>：</p>\n<ul>\n<li>提供更高的性能，具有更好的容错。</li>\n<li>支持复杂和近实时分析。</li>\n<li>实时输出数据流。</li>\n<li>可扩展的插件框架。</li>\n<li>通用解析器。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//irds.usc.edu/sparkler/dev/development-environment-setup.html%23contributing-source\">http://irds.usc.edu/sparkler/dev/development-environment-setup.html#contributing-source</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=http%3A//irds.usc.edu/sparkler/\">http://irds.usc.edu/sparkler/</a></strong></p>\n<h2 id=\"Perl-编写的开源-Web-爬虫\"><a class=\"header-anchor\" href=\"#Perl-编写的开源-Web-爬虫\">※</a><strong>Perl 编写的开源 Web 爬虫</strong></h2>\n<h3 id=\"50-Web-scraper\"><a class=\"header-anchor\" href=\"#50-Web-scraper\">※</a><strong>50. Web-scraper</strong></h3>\n<ul>\n<li><strong>实现语言</strong>：Perl</li>\n<li><strong>GitHub Star 数</strong>： 91</li>\n<li><strong>官方支持链接</strong></li>\n</ul>\n<p><strong>简介</strong>：</p>\n<ul>\n<li>Web Scraper 是一种使用 HTML、CSS 选择器或 XPath 表达式的 Web 采集工具集。</li>\n</ul>\n<p>– <strong>官方文档</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/miyagawa/web-scraper\">https://github.com/miyagawa/web-scraper</a></strong></p>\n<p>– <strong>官方网站</strong>： <strong><a href=\"https://link.zhihu.com/?target=https%3A//github.com/miyagawa/web-scraper\">https://github.com/miyagawa/web-scraper</a></strong></p>\n<h2 id=\"小结\"><a class=\"header-anchor\" href=\"#小结\">※</a><strong>小结</strong></h2>\n<p>以上罗列了 50 个不同编程语言下的不错爬虫框架/项目，感兴趣可以用用看。</p>\n<p><strong>英文原文：</strong> <strong><a href=\"https://link.zhihu.com/?target=http%3A//www.prowebscraper.com/blog/50-best-open-source-web-crawlers/\">http://www.prowebscraper.com/blog/50-best-open-source-web-crawlers/</a></strong></p>\n<p>本文转自 <a href=\"https://zhuanlan.zhihu.com/p/64305013\">https://zhuanlan.zhihu.com/p/64305013</a>，如有侵权，请联系删除。</p>\n","_path":"20220609/50-chong-zui-bang-de-kai-yuan-pa-chong-kuang-jia-xiang-mu/","_link":"https://yaoqs.github.io/20220609/50-chong-zui-bang-de-kai-yuan-pa-chong-kuang-jia-xiang-mu/","_id":"clzpq9hrt0005sger47lzev90"}}